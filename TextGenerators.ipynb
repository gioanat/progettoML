{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGenerators (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fda830aa65243b8bd0dbd42db653a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_049c8d22c15d4a58a4daa9a892ba13c1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a329ddab8b1b4b3d99ae0281c512ce33",
              "IPY_MODEL_47bb67cb1b02457081073b31876d8518"
            ]
          }
        },
        "049c8d22c15d4a58a4daa9a892ba13c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a329ddab8b1b4b3d99ae0281c512ce33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1a702cdaf4074b8dae0fdae64655e98e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9143470,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9143470,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0fa31d010cf48d7b4836f9a3dfe5020"
          }
        },
        "47bb67cb1b02457081073b31876d8518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e9911b34654d4dde82c378e31c3eb908",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.14M/9.14M [00:02&lt;00:00, 3.61MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6ec0648ba8a4525b9822397dce9222a"
          }
        },
        "1a702cdaf4074b8dae0fdae64655e98e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0fa31d010cf48d7b4836f9a3dfe5020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9911b34654d4dde82c378e31c3eb908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6ec0648ba8a4525b9822397dce9222a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e8d719eb698d4cf8820ac0d1a744d8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d81d232931514570bd6f74d99831de1d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cab6c865e0d94a90bf98d6964bd4dd57",
              "IPY_MODEL_0716eec001874e52ab7baa12c4d04ad4"
            ]
          }
        },
        "d81d232931514570bd6f74d99831de1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cab6c865e0d94a90bf98d6964bd4dd57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa6d6d483d0f437da385219f3c8f60c5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9143613,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9143613,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0c372d1c3af4ceebfebac2afa38fe3f"
          }
        },
        "0716eec001874e52ab7baa12c4d04ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7710521407924e7f8db66495fd4dfd94",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.14M/9.14M [00:47&lt;00:00, 194kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4dbd395b0cb142b2848d077e7c990ba9"
          }
        },
        "aa6d6d483d0f437da385219f3c8f60c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0c372d1c3af4ceebfebac2afa38fe3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7710521407924e7f8db66495fd4dfd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4dbd395b0cb142b2848d077e7c990ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48c315236edf467b9c35e3190a7cfb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e6b26c752004edabf57d46f887995f6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57047b4effe94822a29450e45d8615d5",
              "IPY_MODEL_cde084f2abe245c8a34f36275c9e5160"
            ]
          }
        },
        "4e6b26c752004edabf57d46f887995f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57047b4effe94822a29450e45d8615d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a2e84bbdc67844e6b9e9831f2e68df6d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 856,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 856,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbb237f31f7841ebb6f431816d0e87ea"
          }
        },
        "cde084f2abe245c8a34f36275c9e5160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f6ba5539c31b471ca6455e36661fbdec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 856/856 [00:02&lt;00:00, 408B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f080ad1abea0453c910b4b9e5e0ccf36"
          }
        },
        "a2e84bbdc67844e6b9e9831f2e68df6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbb237f31f7841ebb6f431816d0e87ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6ba5539c31b471ca6455e36661fbdec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f080ad1abea0453c910b4b9e5e0ccf36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62e492232c924c049995bda8111cf37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1bc69d47833a4fd4a8b426529eaff5b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6baef10d53d748b9a0c1123fcb906cfe",
              "IPY_MODEL_61860cd62e284be2891036e0f3d5a5a8"
            ]
          }
        },
        "1bc69d47833a4fd4a8b426529eaff5b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6baef10d53d748b9a0c1123fcb906cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b73f2ec93e141b9a1c2151af4bfae08",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1140884800,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1140884800,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_519343c17cac43a1abff3a4e5a75c180"
          }
        },
        "61860cd62e284be2891036e0f3d5a5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_78452bbb6e8e41bdbd84e746a5658f4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.14G/1.14G [00:38&lt;00:00, 29.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31bb7f97f5494a018cd40581583fe3cd"
          }
        },
        "0b73f2ec93e141b9a1c2151af4bfae08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "519343c17cac43a1abff3a4e5a75c180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78452bbb6e8e41bdbd84e746a5658f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31bb7f97f5494a018cd40581583fe3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvNGdUiYGsTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75522368-edc1-4f89-9bf0-420dc43ec262"
      },
      "source": [
        "rm -r progettoML"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'progettoML': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D__X4W3ZN3kn",
        "outputId": "c04f1290-a0ea-4d97-8fb6-c00289a143b6"
      },
      "source": [
        "#!wget https://github.com/git-lfs/git-lfs/releases/download/v2.9.0/git-lfs-linux-amd64-v2.9.0.tar.gz\n",
        "#!tar -xf git-lfs-linux-amd64-v2.9.0.tar.gz\n",
        "#!chmod 755 install.sh\n",
        "#!./install.sh\n",
        "#!git clone -b wikitrain https://github.com/gioanat/progettoML\n",
        "!git clone https://github.com/gioanat/progettoML"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'progettoML'...\n",
            "remote: Enumerating objects: 522, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 522 (delta 57), reused 0 (delta 0), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (522/522), 447.57 MiB | 27.42 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n",
            "Checking out files: 100% (317/317), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfApi3WlL04G",
        "outputId": "d5d48b31-0771-4100-ad50-289c69ab3c40"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "if not os.path.isdir(\"data/wikitext-103/\"):\n",
        "    print(\"Downloading data...\")\n",
        "    subprocess.run(\"wget -c https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip -P data\".split())\n",
        "    print(\"Unzipping data...\")\n",
        "    subprocess.run([\"unzip\", \"data/wikitext-103-v1.zip\", \"-d\", \"data/\"])\n",
        "    print(\"Done...\")\n",
        "else:\n",
        "    print(\"Found data...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data...\n",
            "Unzipping data...\n",
            "Done...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ0OJoUPwi4f"
      },
      "source": [
        "class TableHelper(object):\n",
        "    def __init__(self, row_count: int, col_count: int):\n",
        "        self.grid = [[None for _ in range(col_count)]\n",
        "                     for _ in range(row_count)]\n",
        "\n",
        "    #  column_name permette di indicizzare la colonna a partire dal valore della cella [0]\n",
        "    # se è definito, ha priorità più alta rispetto a column_index\n",
        "    def set_column(self, column_index: int,  items: list, col_name=None):\n",
        "        if col_name != None:\n",
        "            temp = -1\n",
        "            for i in range(len(self.grid[0])):\n",
        "                if self.grid[0][i] == col_name:\n",
        "                    temp = i\n",
        "                    break\n",
        "            if temp == -1:\n",
        "                print(\"Non esiste nessuna colonna chiamata:\", col_name)\n",
        "            column_index = temp\n",
        "        for row_index in range(min(len(self.grid), len(items))):\n",
        "            self.grid[row_index][column_index] = items[row_index]\n",
        "\n",
        "    def set_row(self, row_index: int, items: list, row_name=None):\n",
        "        if row_name != None:\n",
        "            temp = -1\n",
        "            for i in range(len(self.grid)):\n",
        "                if self.grid[i][0] == row_name:\n",
        "                    temp = i\n",
        "                    break\n",
        "            if temp == -1:\n",
        "                print(\"Non esiste nessuna riga chiamata:\", row_name)\n",
        "            row_index = temp\n",
        "            \n",
        "        if len(items) != len(self.grid[0]):\n",
        "            raise Exception(\"Non posso aggiungere un numero di elementi diverso dalla grandezza della tabella: {} != {}\".format(\n",
        "                len(items), len(self.grid[0])))\n",
        "\n",
        "        self.grid[row_index] = items\n",
        "\n",
        "    def set_cell(self,  item: object, row_name=None, col_name=None, row_index=None, col_index=None):\n",
        "        if col_name == None and col_index == None:\n",
        "            raise Exception(\n",
        "                \"Almeno uno fra col_name e col_index dev'essere valorizzato\")\n",
        "\n",
        "        if row_name == None and row_index == None:\n",
        "            raise Exception(\n",
        "                \"Almeno uno fra row_name e row_index dev'essere valorizzato\")\n",
        "\n",
        "        if row_name != None:\n",
        "            temp = -1\n",
        "            for i in range(len(self.grid)):\n",
        "                if self.grid[i][0] == row_name:\n",
        "                    temp = i\n",
        "                    break\n",
        "            if temp == -1:\n",
        "                print(\"Non esiste nessuna riga chiamata:\", row_name)\n",
        "            row_index = temp\n",
        "\n",
        "        if col_name != None:\n",
        "            temp = -1\n",
        "            for i in range(len(self.grid[0])):\n",
        "                if self.grid[0][i] == col_name:\n",
        "                    temp = i\n",
        "                    break\n",
        "            if temp == -1:\n",
        "                print(\"Non esiste nessuna colonna chiamata:\", col_name)\n",
        "            col_index = temp\n",
        "\n",
        "        if col_index == None and row_index == None:\n",
        "            raise Exception(\"Non è possibile aggiungere la cella ({},{}) o ({},{})\".format(\n",
        "                row_index, col_index, row_name, col_name))\n",
        "\n",
        "        if 0 <= row_index < len(self.grid):\n",
        "            if 0 <= col_index < len(self.grid[0]):\n",
        "                self.grid[row_index][col_index] = item\n",
        "                return\n",
        "\n",
        "        raise Exception(\n",
        "            \"La cella ({}, {}) non è presente nella tabella\".format(row_index, col_index))\n",
        "\n",
        "    def print(self):\n",
        "        max_length = 0\n",
        "        for row_index in range(len(self.grid)):\n",
        "            for col_index in range(len(self.grid[row_index])):\n",
        "                cell = self.grid[row_index][col_index]\n",
        "                if cell is None:\n",
        "                    max_length = max(max_length, 4)\n",
        "                    self.grid[row_index][col_index] = ''\n",
        "                else:\n",
        "                    max_length = max(max_length, len(str(cell)))\n",
        "\n",
        "        size = max_length * len(self.grid[0]) + (len(self.grid[0]) + 1)\n",
        "\n",
        "        print('-'*size)\n",
        "        for row in self.grid:\n",
        "            format = \"\"\n",
        "            [print(format+f'|{{:^{max_length}}}'.format(item), end=\"\")\n",
        "             for item in row]\n",
        "            print(\"|\")\n",
        "        print('-'*size)\n",
        "\n",
        "\n",
        "table = TableHelper(row_count=5, col_count=5)\n",
        "table.set_column(column_index=0, items=[\n",
        "                 None, 'BERT base', 'GPT', 'TFXL', 'WT103'])\n",
        "table.set_cell(row_index=0, col_index=1, item='Corpus-BLEU WT103')\n",
        "table.set_cell(row_index=0, col_index=2, item='PPL')\n",
        "table.set_cell(row_index=0, col_index=3, item='Corpus-PPL')\n",
        "table.set_cell(row_index=0, col_index=4, item='EmbSim WT103')\n",
        "\n",
        "\n",
        "#table.print()\n",
        "\n",
        "gramTable = TableHelper(row_count=5, col_count=8)\n",
        "gramTable.set_column(column_index=0, items=[\n",
        "    None, 'BERT base','GPT', 'TFXL', 'WT103'])\n",
        "gramTable.set_cell(row_index=0, col_index=1, item='Self-BLEU')\n",
        "gramTable.set_cell(row_index=0, col_index=2, item='Self unique 2-grams')\n",
        "gramTable.set_cell(row_index=0, col_index=3, item='Self unique 3-grams')\n",
        "gramTable.set_cell(row_index=0, col_index=4, item='Self unique 4-grams')\n",
        "gramTable.set_cell(row_index=0, col_index=5, item='WT103 unique 2-grams')\n",
        "gramTable.set_cell(row_index=0, col_index=6, item='WT103 unique 3-grams')\n",
        "gramTable.set_cell(row_index=0, col_index=7, item='WT103 unique 4-grams')\n",
        "\n",
        "#gramTable.set_cell(item=\"prova\", row_name=\"GPT\",col_name=\"Self unique 3-grams\")\n",
        "\n",
        "#gramTable.print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0_4v5nWaFBD"
      },
      "source": [
        "tableMLE = TableHelper(row_count=3, col_count=5)\n",
        "tableMLE.set_column(column_index=0, items=[\n",
        "                 None, 'MLE', 'WT103MLE'])\n",
        "tableMLE.set_cell(row_index=0, col_index=1, item='Corpus-BLEU WT103')\n",
        "tableMLE.set_cell(row_index=0, col_index=2, item='PPL')\n",
        "tableMLE.set_cell(row_index=0, col_index=3, item='Corpus-PPL')\n",
        "tableMLE.set_cell(row_index=0, col_index=4, item='EmbSim')\n",
        "\n",
        "\n",
        "#table.print()\n",
        "\n",
        "gramTableMLE = TableHelper(row_count=3, col_count=8)\n",
        "gramTableMLE.set_column(column_index=0, items=[\n",
        "    None, 'MLE', 'WT103MLE'])\n",
        "gramTableMLE.set_cell(row_index=0, col_index=1, item='Self-BLEU')\n",
        "gramTableMLE.set_cell(row_index=0, col_index=2, item='Self unique 2-grams')\n",
        "gramTableMLE.set_cell(row_index=0, col_index=3, item='Self unique 3-grams')\n",
        "gramTableMLE.set_cell(row_index=0, col_index=4, item='Self unique 4-grams')\n",
        "gramTableMLE.set_cell(row_index=0, col_index=5, item='WT103 unique 2-grams')\n",
        "gramTableMLE.set_cell(row_index=0, col_index=6, item='WT103 unique 3-grams')\n",
        "gramTableMLE.set_cell(row_index=0, col_index=7, item='WT103 unique 4-grams')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vzi8z1oLRZD"
      },
      "source": [
        "# BERT Has a Mounth and it must speaks:\n",
        "\n",
        "---\n",
        "\n",
        "Importiamo le librerie:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cz8ioMbqlR8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2aae34-8bdc-4e71-95ef-6143edc811f4"
      },
      "source": [
        "try:\n",
        "  from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "except:\n",
        "  import sys\n",
        "  !{sys.executable} -m pip install pytorch_pretrained_bert\n",
        "  from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/96/5c009100d651f4646639f9777a82777430897590f5fd28bbc077cace09bf/boto3-1.17.97-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu102)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.97\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/a2/bd7cd40c61a71c3daede70158b5c0aca918057f63cfbef015c56cccdb6b2/botocore-1.20.97-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.97->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.97->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.97 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.97 botocore-1.20.97 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0LOPf0sMQZr"
      },
      "source": [
        "## BERT-base\n",
        "\n",
        "Carichiamo i pesi del modello 'bert-base-uncased' nel caso di lingua inglese, e 'bert-base-italian-cased' nel caso di lingua italiana. Furthermore, we take tracking of index of special symbols, like [MASK], [CLS] (start of sentences) and [SEP] (separator of two not consecutive sentences)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNl1BYL42Jz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b718cdbf-9248-4319-e01a-bf28b8357c1d"
      },
      "source": [
        "italian = False\n",
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda(0)\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:10<00:00, 39141653.17B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 1195775.24B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3UJ91CdOg39"
      },
      "source": [
        "In this chunk, we define some functions that allow us to obtain the index of the given token in the vocabulary and vice versa and to detokenize generated sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0e6Nhb1MQAk"
      },
      "source": [
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G96Z816-QoEk"
      },
      "source": [
        "This function generates a word and locates it in gen_idx position. Given tensor of logits of the batch, this function generates a new word:\n",
        "\n",
        "\n",
        "\n",
        "*   sampling from the top $k$ most probable words if 'top_k' parameter is different from zero;\n",
        "*   sampling from full distribution if 'sample' is equals to 'True';\n",
        "*   otherwise, the most probable words is chosen.\n",
        "\n",
        "Furthemore, if temperature parameter is given in input, then it is uses with combination of softmax function, for retrieving the probabilities.\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{softmax}(z)_i = \\frac{e^{\\frac{z_i}{T}}}{\\sum_{j=1}^N e^{\\frac{z_j}{T}}}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "So,temperature is a hyperparameter which is applied to logits to affect the final probabilities from the softmax. In particular\n",
        "\n",
        "*   A low temperature (below 1) makes the model more confident.\n",
        "*   A high temperature (above 1) makes the model less confident."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPoq_C0NlR8m"
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    if italian:\n",
        "      logits = out.logits[:,gen_idx]\n",
        "    else:\n",
        "      logits = out[:,gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_Gksz4fhEqO"
      },
      "source": [
        "This is the meat of the algorithm. The general idea is\n",
        "\n",
        "1.   start from all masks (get_init_text);\n",
        "2.   repeatedly pick a location, mask the token at that location, and generate from the probability distribution given by BERT;\n",
        "3.   stop when converged or tired of waiting\n",
        "\n",
        "We consider three \"modes\" of generating:\n",
        "\n",
        "*    generate a single token for a position chosen uniformly at random for a chosen number of time steps (parallel_sequential_generation);\n",
        "*    generate in sequential order (L->R), one token at a time (sequential_generation);\n",
        "*    generate for all positions at once for a chosen number of time steps (parallel_generation).\n",
        "\n",
        "The generate function wraps and batches these three generation modes. In practice, we find that the first leads to the most fluent samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIjQsTJvlR8m"
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]  # crea batch_size sentences of max_len that start with [CLS] and end with [SEP]\n",
        "    #if rand_init:\n",
        "    #    for ii in range(max_len):\n",
        "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
        "    \n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def parallel_sequential_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len) # seleziona un indice casuale tra 0 e max_len e maschera il relativo token in ogni sentences\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def parallel_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
        "                        cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for all positions at each time step \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        for kk in range(max_len):\n",
        "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
        "            for jj in range(batch_size):\n",
        "                batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        #if verbose and np.mod(ii, print_every) == 0:\n",
        "        #    print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
        "    \n",
        "    return untokenize_batch(batch)\n",
        "            \n",
        "def sequential_generation(seed_text, batch_size=10, max_len=15, leed_out_len=15, \n",
        "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
        "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size) # batch_size sentences of max_len length with all mask, starting with [CLS] and separating by [SEP]\n",
        "    \n",
        "    for ii in range(max_len):\n",
        "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp) # ricava i valori logits\n",
        "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+ii] = idxs[jj]\n",
        "        \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             generation_mode=\"parallel-sequential\",\n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size) # approssima il rapporto a un numero intero e ripete il processo di generazione di batch_size sentences of length len_max n_batches times.\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        if generation_mode == \"parallel-sequential\":\n",
        "            batch = parallel_sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k,\n",
        "                                                   temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                                   cuda=cuda, verbose=False)\n",
        "        elif generation_mode == \"sequential\":\n",
        "            batch = sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k, \n",
        "                                          temperature=temperature, leed_out_len=leed_out_len, sample=sample,\n",
        "                                          cuda=cuda)\n",
        "        elif generation_mode == \"parallel\":\n",
        "            batch = parallel_generation(seed_text, batch_size=batch_size,\n",
        "                                        max_len=max_len, top_k=top_k, temperature=temperature, \n",
        "                                        sample=sample, max_iter=max_iter, \n",
        "                                        cuda=cuda, verbose=True)\n",
        "        \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QonatV8lR8o"
      },
      "source": [
        "# Utility functions\n",
        "\n",
        "def printer(sent, should_detokenize=True,otherModes =False):\n",
        "    if otherModes:\n",
        "        sent = detokenize(sent[1:-1])\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)\n",
        "    print(\" \".join(sent))\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False,openaiFlag = True):\n",
        "    if openaiFlag:\n",
        "      sents = [sent.replace(\"<EOS> \",\"\").strip().split() for sent in open(in_file).readlines()]\n",
        "      if should_detokenize:\n",
        "          sents = [detokenize(sent) for sent in sents]\n",
        "      return sents\n",
        "    else:\n",
        "      sents = [sent.replace(\"<unk>\",\"[UNK]\").strip().split() for sent in open(in_file).readlines()]\n",
        "      if should_detokenize:\n",
        "          sents = [detokenize(sent) for sent in sents]\n",
        "      return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False,bert=True):\n",
        "    with open(out_file, \"w\") as out_fh:\n",
        "        for sent in sents:\n",
        "          if bert:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "          else:  \n",
        "            sent = detokenize(sent) if should_detokenize else sent\n",
        "          out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1h0t-hq0bS2"
      },
      "source": [
        "Let's call the actual generation function! We must set the following:\n",
        "\n",
        "- batch_size: number of generated sentences;\n",
        "- max_len: max length of sequence to generate (equals to number of generated tokens)\n",
        "- n_samples: it generates n_samples/batch_size (rounded) block of batch_size sentences of length max_len (only for 'sequential' mode);\n",
        "- top_k: at each step, sample from the top_k most likely words;\n",
        "- sample: if 'True' sampling from full distribution;\n",
        "- temperature: smoothing parameter for the next word distribution. Higher means more like uniform; lower means more peaky;\n",
        "- burnin: for parallel-sequential generation, for the first burnin steps, sample from the entire next word distribution, instead of top_k;\n",
        "- generation_mode: the desired mode of generation;\n",
        "- leed_out_len: for sequantial-mode only. It is the span of right window;\n",
        "- max_iter: number of iterations to run for in parallel or parallel-sequantial mode;\n",
        "- seed_text ([\"CLS\"]): prefix to generate for. It is crucial to start with the CLS token, but you can try adding to it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpDWSyCdlR8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffffcc6-3d88-4f79-b94a-33319b5b9205"
      },
      "source": [
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = [1.0]\n",
        "generation_mode = \"parallel-sequential\"\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "for temp in temperature:\n",
        "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                      generation_mode=generation_mode,\n",
        "                      sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                      cuda=cuda)\n",
        "    #out_file_BERT = \"%s-len%d-burnin%d-topk%d-temp%.3f-maxIter%d.txt\" % (model_version, max_len, burnin, top_k, temp,max_iter)\n",
        "    #write_sents(out_file_BERT, bert_sents, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished batch 1 in 96.193s\n",
            "Finished batch 2 in 96.405s\n",
            "Finished batch 3 in 96.541s\n",
            "Finished batch 4 in 96.781s\n",
            "Finished batch 5 in 96.795s\n",
            "Finished batch 6 in 96.780s\n",
            "Finished batch 7 in 96.767s\n",
            "Finished batch 8 in 96.748s\n",
            "Finished batch 9 in 96.601s\n",
            "Finished batch 10 in 96.534s\n",
            "Finished batch 11 in 96.524s\n",
            "Finished batch 12 in 96.531s\n",
            "Finished batch 13 in 96.652s\n",
            "Finished batch 14 in 96.725s\n",
            "Finished batch 15 in 96.726s\n",
            "Finished batch 16 in 96.750s\n",
            "Finished batch 17 in 96.608s\n",
            "Finished batch 18 in 96.753s\n",
            "Finished batch 19 in 96.482s\n",
            "Finished batch 20 in 96.814s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgoKkQp-lR8p"
      },
      "source": [
        "out_file_BERT = \"progettoML/save/bert/bert-base-uncased-len40-burnin250-topk100-temp1.000-maxIter500.txt\"\n",
        "bert_sents_out = read_sents(out_file_BERT, should_detokenize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxiFg6u6czZz",
        "outputId": "eca4d1ed-b219-4fdb-e4a7-d91aed7584bc"
      },
      "source": [
        "for sent in bert_sents_out[0:5]:\n",
        "  printer(sent, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1998 ; instrument landing mission \" x , x or x - 6p \" in vanguard - 2 unmanned iss challenger explorer , 1997 , nov . 1997 ; breakthrough mission award winner , december 1996 , jan .\n",
            "stage names ( 4th act ) , generally called \" the touring company \" , include : allan taynt , arthur melford , frederick coombs \" cannonball ! \" as o \" m \" .\n",
            "they walked toward st . cloud , hoping that \" la petite gourde \" would arrive and ask more questions about the battlefield . the traverse - marie journal carried an account of all the sightings .\n",
            "head coach : lee dong - won assistant coach : kwon taeyeon by korea institute of football coaches scheme . awarded red card 2016 for former infirm and mentally ill players by korea football association .\n",
            "buffett was concerned that beershire , then 26 , might not be amused by the brand of new housing ; sunday morning police footage indicated that beershire was carrying a concealed weapon , which she ignored .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSpNWhORdI8_"
      },
      "source": [
        "Now, we tried to use the other sampling modes:\n",
        "\n",
        "1. sequential:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSli-dxtdIRy",
        "outputId": "1405d4ce-6c9b-4f58-840f-d565dd5f5fa2"
      },
      "source": [
        "n_samples = 5\n",
        "batch_size = 5\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 1.0\n",
        "generation_mode = \"sequential\"\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "bert_sents_sequential = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                  generation_mode=generation_mode,\n",
        "                  sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
        "                  cuda=cuda)\n",
        "for sent in bert_sents_sequential:\n",
        "  printer(sent,should_detokenize=True, otherModes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished batch 1 in 1.270s\n",
            "and this eve is now ! everything : everything : perfect : perfect : perfect : perfect : perfect : perfect : perfect : perfect : perfect : perfect : perfect : perfect : perfect : full : perfect !\n",
            "and an old age heart of a different sort , the same feeling , and his own , and of a different kind , a new heart , of a certain sort , perhaps almost both his own kind .\n",
            "some more ... some more ... more ... more ... more ... more ... more ... more ... more ... more ... more ... again ... more ... ... all ... again ... ... ... that was it again .\n",
            "the first in his new group featured in a major television and radio series . after his first few audition appearances in other tv shows and music series including various tv series like love is born and musical star .\n",
            "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * yes , goodnight everybody !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er39kjnYeQqp"
      },
      "source": [
        "2. parallel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCkFEdgneUHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4245600-87c4-4ae0-ebdd-f1f3bf49ec1e"
      },
      "source": [
        "n_samples = 5\n",
        "batch_size = 5\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 1.0\n",
        "generation_mode = \"parallel\"\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 1000\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "bert_sents_parallel = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                  generation_mode=generation_mode,\n",
        "                  sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
        "                  cuda=cuda)\n",
        "for sent in bert_sents_parallel:\n",
        "  printer(sent,should_detokenize=True, otherModes=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished batch 1 in 36.516s\n",
            ". . . . . . let a . call both a - and an . - that - is and , and , , - in - a , and and and . . . . . . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". . . , and that - - - - , as i / , and that . . . . . , that / i / / , - - - - all of that . . . .\n",
            ". . . . , the - and - , - and - , - and - had . or and - the - , and , and - , - and - that . . . . . .\n",
            "| | a - in | | a . ne / in - to : \" and and ... and \" : | for all - and and them - to | for all all and - to . |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6dThZZ6mZPo"
      },
      "source": [
        "### Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8h9lnnffPuO"
      },
      "source": [
        "**Perplexity** This measure is defined as \n",
        "\\begin{equation}\n",
        "    \\operatorname{PPL}=2^{-\\sum_{x} \\tilde{p}(x)\\log_2(q(x))},\n",
        "\\end{equation}\n",
        "where $\\tilde{p}(x)$ is the empirical word distribution of the reference text (used as an approximation of the underlying probability distribution) and $q(x)$ is the probability distribution proposed by the model (\\cite{PPL}). We measure the perplexity of the samples generated by our models by resorting to the additionl language model *transformer\\_lm.wiki103.adaptive* pre-trained on the Wikitext-103 corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "_AYi-B8csP4o",
        "outputId": "68dde7f5-ffc1-465f-aeb7-5b8eabc896c7"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install fastBPE sacremoses\n",
        "!git clone https://github.com/gioanat/fairseq.git\n",
        "!pip install ./fairseq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.45)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.41.1)\n",
            "fatal: destination path 'fairseq' already exists and is not an empty directory.\n",
            "Processing ./fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy; python_version >= \"3.7\" in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (0.29.23)\n",
            "Requirement already satisfied: hydra-core<1.1 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (1.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (2019.12.20)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (2.0.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (1.9.0+cu102)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (1.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (4.41.1)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b6fca67) (1.5.1)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+b6fca67) (5.1.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+b6fca67) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+b6fca67) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+b6fca67) (3.7.4.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+b6fca67) (2.20)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+b6fca67) (2.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq==1.0.0a0+b6fca67) (3.4.1)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+b6fca67-cp37-cp37m-linux_x86_64.whl size=5179761 sha256=c4c03bdc9ed0d9a39552b8161ae175144327f2945f9b8719a14e34633da8a95e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b55ith8c/wheels/94/b2/67/6399f5bcb823dc3a8b1e84965aaae15af9ed863fee98a59129\n",
            "Successfully built fairseq\n",
            "Installing collected packages: fairseq\n",
            "  Found existing installation: fairseq 1.0.0a0+b6fca67\n",
            "    Uninstalling fairseq-1.0.0a0+b6fca67:\n",
            "      Successfully uninstalled fairseq-1.0.0a0+b6fca67\n",
            "Successfully installed fairseq-1.0.0a0+b6fca67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fairseq"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVpNFWMej4RO",
        "outputId": "997bbed6-6e99-42af-c29a-85cb3591ce2a"
      },
      "source": [
        "#Dependencies\n",
        "import torch\n",
        "\n",
        "#Helper method used to get GPU or CPU object for selecting model attachement\n",
        "def get_device():\n",
        "    #Check if there is a GPU available\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print('Using GPU:' + torch.cuda.get_device_name(0))\n",
        "    # Uses the CPU if no GPU available\n",
        "    else:\n",
        "        print('Using CPU.')\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    return device\n",
        "\n",
        "def load_perplexity_model():\n",
        "    print(\"Loading pre-trained transformer_lm.wiki103.adaptive\")\n",
        "    # Load an English LM trained on wiki103 data\n",
        "    en_lm = torch.hub.load('pytorch/fairseq', 'transformer_lm.wiki103.adaptive', tokenizer='moses')\n",
        "    en_lm.requires_grad=False\n",
        "    en_lm.eval()  # disable dropout\n",
        "    print('Finished Loading pre-trained transformer_lm.wiki103.adaptive')\n",
        "    return en_lm\n",
        "\n",
        "\n",
        "device = get_device() #Using for loading models to GPU\n",
        "perplexity_model = load_perplexity_model().to(device) #Outside model used for evaluating perplexity of BERT models\n",
        "print('Finished loading utility models')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU:Tesla T4\n",
            "Loading pre-trained transformer_lm.wiki103.adaptive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Loading pre-trained transformer_lm.wiki103.adaptive\n",
            "Finished loading utility models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z63MiMxYj5Mq",
        "outputId": "5eabfa88-7d6e-4208-ada5-335ea5ec60df"
      },
      "source": [
        "import numpy as np\n",
        "PPL_BERT = np.average([(perplexity_model.score(\" \".join(sent))['positional_scores'].mean().neg().exp()).item() for sent in bert_sents_out])\n",
        "print(\"BERT-base perplexity:\",format(PPL_BERT,\".4f\"))\n",
        "table.set_cell(item=format(PPL_BERT,\".4f\"), row_name=\"BERT base\",col_name=\"PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-base perplexity: 289.1632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb5MwEPbjGP7",
        "outputId": "a86db97f-0904-4a36-c221-fa951e886fb0"
      },
      "source": [
        "PPL_corpus_BERT = [perplexity_model.score(\" \".join(sent))['positional_scores'].mean().item() for sent in bert_sents_out]\n",
        "PPL_corpus_BERT = np.exp(-np.mean(PPL_corpus_BERT))\n",
        "print(\"BERT-base perplexity-corpus:\",format(PPL_corpus_BERT,\".4f\"))\n",
        "table.set_cell(item=format(PPL_corpus_BERT,\".4f\"), row_name=\"BERT base\",col_name=\"Corpus-PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-base perplexity-corpus: 212.9936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOKriqG11gBC"
      },
      "source": [
        "# OpenAI Generative Pre-Training Transformer (GPT)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See this repo by Thomas Wolf at Huggingface for instructions for setting up the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDJMNWs16AuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166d40a9-6047-4b7f-c9fc-2e109e3bd7cd"
      },
      "source": [
        "import re\n",
        "try:\n",
        "  import ftfy\n",
        "except:\n",
        "  import sys\n",
        "  !{sys.executable} -m pip install ftfy\n",
        "  import ftfy\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"\n",
        "    Return set of symbol pairs in a word.\n",
        "    word is represented as tuple of symbols (symbols being variable-length strings)\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "def text_standardize(text):\n",
        "    \"\"\"\n",
        "    fixes some issues the spacy tokenizer had on books corpus\n",
        "    also does some whitespace standardization\n",
        "    \"\"\"\n",
        "    text = text.replace('—', '-')\n",
        "    text = text.replace('–', '-')\n",
        "    text = text.replace('―', '-')\n",
        "    text = text.replace('…', '...')\n",
        "    text = text.replace('´', \"'\")\n",
        "    text = re.sub(r'''(-+|~+|!+|\"+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)''', r' \\1 ', text)\n",
        "    text = re.sub(r'\\s*\\n\\s*', ' \\n ', text)\n",
        "    text = re.sub(r'[^\\S\\n]+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "class TextEncoder(object):\n",
        "    \"\"\"\n",
        "    mostly a wrapper for a public python bpe tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_path, bpe_path):\n",
        "        self.nlp = spacy.load('en', disable=['parser', 'tagger', 'ner', 'textcat'])\n",
        "        self.encoder = json.load(open(encoder_path))\n",
        "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
        "        merges = open(bpe_path, encoding='utf-8').read().split('\\n')[1:-1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {}\n",
        "\n",
        "    def bpe(self, token):\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        pairs = get_pairs(word)\n",
        "\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        if word == '\\n  </w>':\n",
        "            word = '\\n</w>'\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, texts, verbose=True):\n",
        "        texts_tokens = []\n",
        "        if verbose:\n",
        "            for text in tqdm(texts, ncols=80, leave=False):\n",
        "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
        "                text_tokens = []\n",
        "                for token in text:\n",
        "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
        "                texts_tokens.append(text_tokens)\n",
        "        else:\n",
        "            for text in texts:\n",
        "                text = self.nlp(text_standardize(ftfy.fix_text(text)))\n",
        "                text_tokens = []\n",
        "                for token in text:\n",
        "                    text_tokens.extend([self.encoder.get(t, 0) for t in self.bpe(token.text.lower()).split(' ')])\n",
        "                texts_tokens.append(text_tokens)\n",
        "        return texts_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 8.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=b9c4e3006328aa300b246ed3d73cf4dafdaca9cf4eb0a42ae5998e48c48a1892\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEmDLtW08qWo"
      },
      "source": [
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT_FNS = {\n",
        "    'relu': nn.ReLU,\n",
        "    'swish': swish,\n",
        "    'gelu': gelu\n",
        "}\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module in the OpenAI style (epsilon inside the square root).\"\n",
        "\n",
        "    def __init__(self, n_state, e=1e-5):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.g = nn.Parameter(torch.ones(n_state))\n",
        "        self.b = nn.Parameter(torch.zeros(n_state))\n",
        "        self.e = e\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.e)\n",
        "        return self.g * x + self.b\n",
        "\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, rf, nx):\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.rf = rf\n",
        "        self.nf = nf\n",
        "        if rf == 1:  # faster 1x1 conv\n",
        "            w = torch.empty(nx, nf)\n",
        "            nn.init.normal_(w, std=0.02)\n",
        "            self.w = Parameter(w)\n",
        "            self.b = Parameter(torch.zeros(nf))\n",
        "        else:  # was used to train LM\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.rf == 1:\n",
        "            size_out = x.size()[:-1] + (self.nf,)\n",
        "            x = torch.addmm(self.b, x.view(-1, x.size(-1)), self.w)\n",
        "            x = x.view(*size_out)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, nx, n_ctx, cfg, scale=False):\n",
        "        super(Attention, self).__init__()\n",
        "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
        "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
        "        assert n_state % cfg.n_head == 0\n",
        "        self.register_buffer('b', torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
        "        self.n_head = cfg.n_head\n",
        "        self.split_size = n_state\n",
        "        self.scale = scale\n",
        "        self.c_attn = Conv1D(n_state * 3, 1, nx)\n",
        "        self.c_proj = Conv1D(n_state, 1, nx)\n",
        "        self.attn_dropout = nn.Dropout(cfg.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(cfg.resid_pdrop)\n",
        "\n",
        "    def _attn(self, q, k, v):\n",
        "        w = torch.matmul(q, k)\n",
        "        if self.scale:\n",
        "            w = w / math.sqrt(v.size(-1))\n",
        "        # w = w * self.b + -1e9 * (1 - self.b)  # TF implem method: mask_attn_weights\n",
        "        # XD: self.b may be larger than w, so we need to crop it\n",
        "        b = self.b[:, :, :w.size(-2), :w.size(-1)]\n",
        "        w = w * b + -1e9 * (1 - b)\n",
        "\n",
        "        w = nn.Softmax(dim=-1)(w)\n",
        "        w = self.attn_dropout(w)\n",
        "        return torch.matmul(w, v)\n",
        "\n",
        "    def merge_heads(self, x):\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
        "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
        "\n",
        "    def split_heads(self, x, k=False):\n",
        "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
        "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
        "        if k:\n",
        "            return x.permute(0, 2, 3, 1)\n",
        "        else:\n",
        "            return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_attn(x)\n",
        "        query, key, value = x.split(self.split_size, dim=2)\n",
        "        query = self.split_heads(query)\n",
        "        key = self.split_heads(key, k=True)\n",
        "        value = self.split_heads(value)\n",
        "        a = self._attn(query, key, value)\n",
        "        a = self.merge_heads(a)\n",
        "        a = self.c_proj(a)\n",
        "        a = self.resid_dropout(a)\n",
        "        return a\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_state, cfg):  # in MLP: n_state=3072 (4 * n_embd)\n",
        "        super(MLP, self).__init__()\n",
        "        nx = cfg.n_embd\n",
        "        self.c_fc = Conv1D(n_state, 1, nx)\n",
        "        self.c_proj = Conv1D(nx, 1, n_state)\n",
        "        self.act = ACT_FNS[cfg.afn]\n",
        "        self.dropout = nn.Dropout(cfg.resid_pdrop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.act(self.c_fc(x))\n",
        "        h2 = self.c_proj(h)\n",
        "        return self.dropout(h2)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_ctx, cfg, scale=False):\n",
        "        super(Block, self).__init__()\n",
        "        nx = cfg.n_embd\n",
        "        self.attn = Attention(nx, n_ctx, cfg, scale)\n",
        "        self.ln_1 = LayerNorm(nx)\n",
        "        self.mlp = MLP(4 * nx, cfg)\n",
        "        self.ln_2 = LayerNorm(nx)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.attn(x)\n",
        "        n = self.ln_1(x + a)\n",
        "        m = self.mlp(n)\n",
        "        h = self.ln_2(n + m)\n",
        "        return h\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\" Transformer model \"\"\"\n",
        "\n",
        "    def __init__(self, cfg, vocab=40990, n_ctx=512):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.vocab = vocab\n",
        "        self.embed = nn.Embedding(vocab, cfg.n_embd)\n",
        "        self.drop = nn.Dropout(cfg.embd_pdrop)\n",
        "        block = Block(n_ctx, cfg, scale=True)\n",
        "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(cfg.n_layer)])\n",
        "\n",
        "        nn.init.normal_(self.embed.weight, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, x.size(-2), x.size(-1))\n",
        "        e = self.embed(x)\n",
        "        # Add the position information to the input embeddings\n",
        "        h = e.sum(dim=2)\n",
        "        for block in self.h:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LMHead(nn.Module):\n",
        "    \"\"\" Language Model Head for the transformer \"\"\"\n",
        "\n",
        "    def __init__(self, model, cfg, trunc_and_reshape=True):\n",
        "        super(LMHead, self).__init__()\n",
        "        self.n_embd = cfg.n_embd\n",
        "        embed_shape = model.embed.weight.shape\n",
        "        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)\n",
        "        self.decoder.weight = model.embed.weight # Tied weights\n",
        "        self.trunc_and_reshape = trunc_and_reshape  # XD\n",
        "\n",
        "    def forward(self, h):\n",
        "        # Truncated Language modeling logits (we remove the last token)\n",
        "        h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd) \\\n",
        "            if self.trunc_and_reshape else h  # XD\n",
        "        lm_logits = self.decoder(h_trunc)\n",
        "        return lm_logits\n",
        "\n",
        "\n",
        "class MultipleChoiceHead(nn.Module):\n",
        "    \"\"\" Classifier Head for the transformer \"\"\"\n",
        "\n",
        "    def __init__(self, clf_token, cfg):\n",
        "        super(MultipleChoiceHead, self).__init__()\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.clf_token = clf_token\n",
        "        self.dropout = nn.Dropout2d(cfg.clf_pdrop)  # To reproduce the noise_shape parameter of TF implementation\n",
        "        self.linear = nn.Linear(cfg.n_embd, 1)\n",
        "\n",
        "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, h, x):\n",
        "        # Classification logits\n",
        "        clf_h = h.view(-1, self.n_embd)\n",
        "        flat = x[..., 0].contiguous().view(-1)\n",
        "        clf_h = clf_h[flat == self.clf_token, :]\n",
        "        clf_h = clf_h.view(-1, x.size(1), self.n_embd, 1)\n",
        "        # This double transposition is there to replicate the behavior\n",
        "        # of the noise_shape argument in the tensorflow\n",
        "        # implementation.  For more details, see\n",
        "        # https://github.com/huggingface/pytorch-openai-transformer-lm/issues/11\n",
        "        clf_h = self.dropout(clf_h.transpose(1, 2)).transpose(1, 2)\n",
        "        clf_h = clf_h.contiguous().view(-1, self.n_embd)\n",
        "        clf_logits = self.linear(clf_h)\n",
        "\n",
        "        return clf_logits.view(-1, x.size(1))\n",
        "\n",
        "\n",
        "class ClfHead(nn.Module):\n",
        "    \"\"\"Classification Head for the transformer\n",
        "\n",
        "    TODO: test this class.\"\"\"\n",
        "    def __init__(self, clf_token, cfg, n_class):\n",
        "        super(ClfHead, self).__init__()\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.clf_token = clf_token\n",
        "        self.dropout = nn.Dropout(cfg.clf_pdrop)\n",
        "        self.linear = nn.Linear(cfg.n_embd, n_class)\n",
        "\n",
        "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, h, x):\n",
        "        clf_h = h.view(-1, self.n_embd)\n",
        "        flat = x[..., 0].contiguous().view(-1)\n",
        "        clf_h = clf_h[flat == self.clf_token, :]\n",
        "        clf_h = self.dropout(clf_h)\n",
        "        clf_logits = self.linear(clf_h)\n",
        "\n",
        "        return clf_logits\n",
        "\n",
        "class SimilarityHead(nn.Module):\n",
        "    \"\"\" Similarity Head for the transformer\n",
        "\n",
        "        TODO: test this class.\"\"\"\n",
        "    def __init__(self, clf_token, cfg):\n",
        "        super(SimilarityHead, self).__init__()\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.clf_token = clf_token\n",
        "        self.dropout = nn.Dropout(cfg.clf_pdrop)\n",
        "        self.linear = nn.Linear(cfg.n_embd, 1)\n",
        "\n",
        "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
        "        nn.init.normal_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, h, x):\n",
        "        sim_h = h.view(-1, self.n_embd)\n",
        "        flat = x[..., 0].contiguous().view(-1)\n",
        "        sim_h = sim_h[flat == self.clf_token, :]\n",
        "        sim_h = self.dropout(sim_h)\n",
        "        sim_h = sim_h.sum(dim = 1)\n",
        "        sim_logits = self.linear(sim_h)\n",
        "\n",
        "        return sim_logits\n",
        "\n",
        "\n",
        "# XD\n",
        "class LMModel(nn.Module):\n",
        "    \"\"\" Transformer with language model head only \"\"\"\n",
        "    def __init__(self, cfg, vocab=40990, n_ctx=512, return_probs=False):\n",
        "        super(LMModel, self).__init__()\n",
        "        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n",
        "        self.lm_head = LMHead(self.transformer, cfg, trunc_and_reshape=False)\n",
        "        self.return_probs = return_probs\n",
        "        if self.return_probs:\n",
        "            pos_emb_mask = torch.zeros(1, 1, vocab)\n",
        "            pos_emb_mask[:, :, -n_ctx:] = -1e12\n",
        "            self.register_buffer('pos_emb_mask', pos_emb_mask)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.transformer(x)\n",
        "        lm_logits = self.lm_head(h)\n",
        "        if self.return_probs:\n",
        "            lm_logits = F.softmax(lm_logits + self.pos_emb_mask, dim=-1)\n",
        "        return lm_logits\n",
        "\n",
        "\n",
        "class DoubleHeadModel(nn.Module):\n",
        "    \"\"\" Transformer with language model and task specific heads \"\"\"\n",
        "    def __init__(self, cfg, clf_token, task_head_type, vocab=40990, n_ctx=512):\n",
        "        super(DoubleHeadModel, self).__init__()\n",
        "        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n",
        "        self.lm_head = LMHead(self.transformer, cfg)\n",
        "        if isinstance(task_head_type, str):\n",
        "            if task_head_type == 'multiple_choice':\n",
        "                self.task_head = MultipleChoiceHead(clf_token, cfg)\n",
        "            elif task_head_type == 'similarity':\n",
        "                self.task_head = SimilarityHead(clf_token, cfg)\n",
        "            elif task_head_type == 'inference':\n",
        "                # the three classes correspond to entailment, contradiction and neutral.\n",
        "                self.task_head = ClfHead(clf_token, cfg, 3)\n",
        "            else:\n",
        "                raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
        "                                 \"'similarity', 'inference' or ('classification', n_class) \"\n",
        "                                 f\"got {task_head_type}.\")\n",
        "        elif isinstance(task_head_type, collections.abc.Sequence) and len(task_head_type) == 2 and \\\n",
        "             task_head_type[0] == 'classification':\n",
        "            n_class = task_head_type[1]\n",
        "            self.task_head = ClfHead(clf_token, cfg, n_class)\n",
        "        else:\n",
        "            raise ValueError(\"task_head_type is expected to be 'multiple_choice' \"\n",
        "                             \"'similarity', 'inference' or ('classification', n_class) \"\n",
        "                             f\"got {task_head_type}.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.transformer(x)\n",
        "        lm_logits = self.lm_head(h)\n",
        "        task_logits = self.task_head(h, x)\n",
        "\n",
        "        return lm_logits, task_logits\n",
        "\n",
        "\n",
        "def load_openai_pretrained_model(model, n_ctx=-1, n_special=-1, n_transfer=12, n_embd=768, path='./model-GPT/',\n",
        "                                 path_names='./'):\n",
        "    # Load weights from TF model\n",
        "    print(\"Loading weights...\")\n",
        "    names = json.load(open(path_names + 'parameters_names.json'))\n",
        "    shapes = json.load(open(path + 'params_shapes.json'))\n",
        "    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n",
        "    init_params = [np.load(path + 'params_{}.npy'.format(n)) for n in range(10)]\n",
        "    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n",
        "    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n",
        "    if n_ctx > 0:\n",
        "        init_params[0] = init_params[0][:n_ctx]\n",
        "    if n_special > 0:\n",
        "        init_params[0] = np.concatenate(\n",
        "            [init_params[1],\n",
        "             (np.random.randn(n_special, n_embd) * 0.02).astype(np.float32),\n",
        "             init_params[0]\n",
        "             ], 0)\n",
        "    else:\n",
        "        init_params[0] = np.concatenate(\n",
        "            [init_params[1],\n",
        "             init_params[0]\n",
        "             ], 0)\n",
        "    del init_params[1]\n",
        "    if n_transfer == -1:\n",
        "        n_transfer = 0\n",
        "    else:\n",
        "        n_transfer = 1 + n_transfer * 12\n",
        "    init_params = [arr.squeeze() for arr in init_params]\n",
        "\n",
        "    try:\n",
        "        assert model.embed.weight.shape == init_params[0].shape\n",
        "    except AssertionError as e:\n",
        "        e.args += (model.embed.weight.shape, init_params[0].shape)\n",
        "        raise\n",
        "\n",
        "    model.embed.weight.data = torch.from_numpy(init_params[0])\n",
        "\n",
        "    for name, ip in zip(names[1:n_transfer], init_params[1:n_transfer]):\n",
        "        name = name[6:]  # skip \"model/\"\n",
        "        assert name[-2:] == \":0\"\n",
        "        name = name[:-2]\n",
        "        name = name.split('/')\n",
        "        pointer = model\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r'[A-Za-z]+\\d+', m_name):\n",
        "                l = re.split(r'(\\d+)', m_name)\n",
        "            else:\n",
        "                l = [m_name]\n",
        "            pointer = getattr(pointer, l[0])\n",
        "            if len(l) >= 2:\n",
        "                num = int(l[1])\n",
        "                pointer = pointer[num]\n",
        "        try:\n",
        "            assert pointer.shape == ip.shape\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, ip.shape)\n",
        "            raise\n",
        "        pointer.data = torch.from_numpy(ip)\n",
        "\n",
        "\n",
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "DEFAULT_CONFIG = dotdict({\n",
        "    'n_embd': 768,\n",
        "    'n_head': 12,\n",
        "    'n_layer': 12,\n",
        "    'embd_pdrop': 0.1,\n",
        "    'attn_pdrop': 0.1,\n",
        "    'resid_pdrop': 0.1,\n",
        "    'afn': 'gelu',\n",
        "    'clf_pdrop': 0.1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SEkWJ0R39J1"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "sys.path.insert(1, os.path.join(\".\", \"pytorch-openai-transformer-lm\"))\n",
        "\n",
        "\n",
        "#from model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
        "#from text_utils import TextEncoder\n",
        "\n",
        "def load_openai_gpt(n_special=1, n_ctx=512):\n",
        "    text_encoder = TextEncoder(\"progettoML/save/model-GPT/encoder_bpe_40000.json\", \n",
        "                               \"progettoML/save/model-GPT/vocab_40000.bpe\")\n",
        "    encoder = text_encoder.encoder\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    vocab = n_vocab + n_special + n_ctx\n",
        "\n",
        "    args = DEFAULT_CONFIG\n",
        "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
        "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
        "                                 path=\"progettoML/save/model-GPT/\",\n",
        "                                 path_names=\"progettoML/save/\")\n",
        "    #lm_model.to(device)\n",
        "    lm_model.return_probs = False\n",
        "    lm_model.eval()\n",
        "    return lm_model, text_encoder\n",
        "\n",
        "def make_batch(X, n_vocab, n_special, batch_size):\n",
        "    X = np.array(X)\n",
        "    assert X.ndim in [1, 2]\n",
        "    if X.ndim == 1:\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
        "    pos_enc = np.tile(pos_enc, (batch_size, pos_enc.shape[-1])) #np.expand_dims(pos_enc, axis=0)\n",
        "    batch = np.stack([X, pos_enc], axis=-1)\n",
        "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
        "    return batch\n",
        "\n",
        "def append_batch(X, next_idx):\n",
        "    next_pos = X[:, -1:, 1] + 1\n",
        "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
        "    return torch.cat((X, next_x), 1)\n",
        "\n",
        "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
        "                             topk=100, sample=True, n_special=0):\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
        "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
        "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
        "    sents = [[] for _ in range(batch_size)]\n",
        "    if seed_text:\n",
        "        seed_ids = text_encoder.encode([seed_text,])\n",
        "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
        "        sents = [[seed_text] for _ in range(batch_size)]\n",
        "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    for step_n in range(gen_len):\n",
        "        out = model(XMB) + model.pos_emb_mask\n",
        "        next_idxs = generate_step(out, gen_idx=step_n, top_k=topk, sample=sample, return_list=False)\n",
        "        idxs = next_idxs.tolist()\n",
        "        for i in range(batch_size):\n",
        "            next_token = idxs[i]\n",
        "            if next_token == n_vocab:\n",
        "                next_token = \"<EOS>\"\n",
        "            else:\n",
        "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
        "            sents[i].append(next_token)\n",
        "        XMB = append_batch(XMB, next_idxs.unsqueeze(-1))\n",
        "        \n",
        "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
        "\n",
        "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
        "                    batch_size=10, gen_len=20, \n",
        "                    topk=100, temperature=1.00, sample=False,\n",
        "                    n_special=0, print_every=1):\n",
        "    sents = []\n",
        "    start_time = time.time()\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    for batch_n in range(n_batches):\n",
        "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
        "                                                batch_size=batch_size, gen_len=gen_len, \n",
        "                                                topk=topk, sample=sample,\n",
        "                                                n_special=n_special)\n",
        "        sents += batch_sents\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "    return sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1IZxYWt4-bB",
        "outputId": "ff69dcc4-ce43-446d-9a0e-9e27d4c9e8d8"
      },
      "source": [
        "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-udo9dVAHJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0cfcb3-bfb0-4c50-a9ca-fd833a76123a"
      },
      "source": [
        "italian = False\n",
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 1.0\n",
        "sample = True\n",
        "\n",
        "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"\", \n",
        "                               n_samples=n_samples, batch_size=batch_size, gen_len=max_len,\n",
        "                               topk=top_k, temperature=temperature, sample=sample,\n",
        "                               n_special=1, print_every=1)\n",
        "\n",
        "#out_file_OPENAI = \"%s-len%d-topk%d-temp%.3f-num_gen%d.txt\" % (\"openai-english\", max_len, top_k, temperature,4)\n",
        "\n",
        "#write_sents(out_file_OPENAI, openai_sents, should_detokenize=True,bert=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated batch 1 of 20 in 142.703s\n",
            "Generated batch 2 of 20 in 143.443s\n",
            "Generated batch 3 of 20 in 144.359s\n",
            "Generated batch 4 of 20 in 144.338s\n",
            "Generated batch 5 of 20 in 144.874s\n",
            "Generated batch 6 of 20 in 143.747s\n",
            "Generated batch 7 of 20 in 144.397s\n",
            "Generated batch 8 of 20 in 143.724s\n",
            "Generated batch 9 of 20 in 142.623s\n",
            "Generated batch 10 of 20 in 142.718s\n",
            "Generated batch 11 of 20 in 141.889s\n",
            "Generated batch 12 of 20 in 142.461s\n",
            "Generated batch 13 of 20 in 141.690s\n",
            "Generated batch 14 of 20 in 142.257s\n",
            "Generated batch 15 of 20 in 142.117s\n",
            "Generated batch 16 of 20 in 142.549s\n",
            "Generated batch 17 of 20 in 142.757s\n",
            "Generated batch 18 of 20 in 141.777s\n",
            "Generated batch 19 of 20 in 144.120s\n",
            "Generated batch 20 of 20 in 143.090s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h8WDECxB0Ym",
        "outputId": "70a3fce4-24ef-46ec-eacd-8b3d4051c2ac"
      },
      "source": [
        "out_file_OPENAI = \"progettoML/save/openai/openai-english-len40-topk100-temp1.000.txt\"\n",
        "openai_sents_out = read_sents(out_file_OPENAI, should_detokenize=False,openaiFlag = True)\n",
        "for sent in openai_sents_out[0:5]:\n",
        "    printer(sent, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" but ... this is important , \" a guy called out from above . \" we do n't know if we want to get arrested , \" he said . \" we 'll be okay , \" a\n",
            "i want to be alone . i would rather avoid the people who know me only by breathing in their fear . do n't they want to know what it 's like to lose friends that they 've known for\n",
            "\" no , i 'm okay . \" she tries to sit up again . this time when she looks down at herself she realizes that she 's covered with dirt and blood streaked on her clothing . she pulls\n",
            "but that 's not all . \" cassie 's words dripped with disapproval . \" they do n't have to work so hard or think so much . what they can do is just like their father , and this\n",
            "she 'd learned that he used words for the pleasure she was giving him . he let her touch him and wondered what her body looked like . i wish i could see it ... but he\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEAosIiYrfyQ",
        "outputId": "234f943a-f932-4912-cac3-4ea59285dd7d"
      },
      "source": [
        "PPL_openai = np.average([(perplexity_model.score(\" \".join(sent))['positional_scores'].mean().neg().exp()).item() for sent in openai_sents_out])\n",
        "print(\"OpenAI-GPT perplexity:\",format(PPL_openai,\".4f\"))\n",
        "table.set_cell(item=format(PPL_openai,\".4f\"), row_name=\"GPT\",col_name=\"PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenAI-GPT perplexity: 175.7630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6njIF892j5A4",
        "outputId": "aa2a3a1f-2f94-4c94-c5ea-6be6f99e9523"
      },
      "source": [
        "PPL_openai_corpus = [perplexity_model.score(\" \".join(sent))['positional_scores'].mean().item() for sent in openai_sents_out]\n",
        "PPL_openai_corpus = np.exp(-np.mean(PPL_openai_corpus))\n",
        "print(\"OpenAI GPT perplexity-corpus:\",format(PPL_openai_corpus,\".4f\"))\n",
        "table.set_cell(item=format(PPL_openai_corpus,\".4f\"), row_name=\"GPT\",col_name=\"Corpus-PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenAI GPT perplexity-corpus: 155.2120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZftOC11zKsVq"
      },
      "source": [
        "# Transformer XL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT-ZWOMQoYbY"
      },
      "source": [
        "*Transformer-XL* is a neural architecture that contrarily to previous models allows to learn token dependency with- out disrupting temporal coherence. Its structure is almost identical to the one of GPT, except for the fact that it introduces a recurrence mechanism for two consecutive segments (a number of consecutive tokens), similarly to what a basic RNN with two consecutive inputs does. Tokens are sequentially fed to the model and at each step the current input is concatenated with the hidden state of the previous segment in order to compute the attention scores. This mechanism enables the model to retain past information. This new implementation requires a relative positional encoder to be used rather than a traditional one. This is done to allow the model to better capture the positional difference that elapse between two distinct words in a segment.\n",
        "Transformer-XL uses a word level tokenizer. \n",
        "The model was trained on the Wikitext-103 corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRF9LjGEjEAl",
        "outputId": "2690b13d-8046-4dc5-9ca7-3d754994bc47"
      },
      "source": [
        "import torch\n",
        "try:\n",
        "  from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer\n",
        "except:\n",
        "  import sys\n",
        "  !{sys.executable} -m pip install transformers\n",
        "  from transformers import TransfoXLLMHeadModel, TransfoXLTokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DbF2o623DUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "5fda830aa65243b8bd0dbd42db653a43",
            "049c8d22c15d4a58a4daa9a892ba13c1",
            "a329ddab8b1b4b3d99ae0281c512ce33",
            "47bb67cb1b02457081073b31876d8518",
            "1a702cdaf4074b8dae0fdae64655e98e",
            "c0fa31d010cf48d7b4836f9a3dfe5020",
            "e9911b34654d4dde82c378e31c3eb908",
            "b6ec0648ba8a4525b9822397dce9222a",
            "e8d719eb698d4cf8820ac0d1a744d8b0",
            "d81d232931514570bd6f74d99831de1d",
            "cab6c865e0d94a90bf98d6964bd4dd57",
            "0716eec001874e52ab7baa12c4d04ad4",
            "aa6d6d483d0f437da385219f3c8f60c5",
            "c0c372d1c3af4ceebfebac2afa38fe3f",
            "7710521407924e7f8db66495fd4dfd94",
            "4dbd395b0cb142b2848d077e7c990ba9",
            "48c315236edf467b9c35e3190a7cfb85",
            "4e6b26c752004edabf57d46f887995f6",
            "57047b4effe94822a29450e45d8615d5",
            "cde084f2abe245c8a34f36275c9e5160",
            "a2e84bbdc67844e6b9e9831f2e68df6d",
            "fbb237f31f7841ebb6f431816d0e87ea",
            "f6ba5539c31b471ca6455e36661fbdec",
            "f080ad1abea0453c910b4b9e5e0ccf36",
            "62e492232c924c049995bda8111cf37f",
            "1bc69d47833a4fd4a8b426529eaff5b2",
            "6baef10d53d748b9a0c1123fcb906cfe",
            "61860cd62e284be2891036e0f3d5a5a8",
            "0b73f2ec93e141b9a1c2151af4bfae08",
            "519343c17cac43a1abff3a4e5a75c180",
            "78452bbb6e8e41bdbd84e746a5658f4b",
            "31bb7f97f5494a018cd40581583fe3cd"
          ]
        },
        "outputId": "274ade44-624e-419e-c37d-8946ab3cb967"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
        "model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103').to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fda830aa65243b8bd0dbd42db653a43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9143470.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8d719eb698d4cf8820ac0d1a744d8b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9143613.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48c315236edf467b9c35e3190a7cfb85",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=856.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62e492232c924c049995bda8111cf37f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1140884800.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTxq4JsIXJMK"
      },
      "source": [
        "Selecting starter lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSpdQf0mXJML"
      },
      "source": [
        "import re\n",
        "from numpy.random import default_rng\n",
        "\n",
        "def replace(sentences: list):\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.replace(\"<eos>\", \"\")\n",
        "        #sentence = re.sub(r\"@(.)@\",r\"\\1\", sentence)\n",
        "        new_sentences.append(sentence.strip())\n",
        "    return new_sentences\n",
        "\n",
        "\n",
        "# [lines_to_pick] indica il numero di righe da restituire\n",
        "# Se supera il numero massimo di righe che sono state preprocessate le ritorna tutte\n",
        "# Se lines_to_pick == -1 le prende tutte in ogni caso\n",
        "def preprocess_file_emnlp(filename: str, lines_to_pick=-1, min_line_length=7, max_line_length=100):\n",
        "    content=list()\n",
        "    with open(filename, 'r') as f:\n",
        "        content = f.readlines()\n",
        "    without_newline = (\"\".join(content)).split(\"\\n\")\n",
        "    print(\"Splitted on new line\")\n",
        "    no_empty_string = [line.strip() for line in without_newline if len(\n",
        "        line.split(\" \")) > 1 and len(line) != 1]\n",
        "    print(\"Removed empty strings\")\n",
        "    count = len(no_empty_string)\n",
        "    if lines_to_pick == -1 or lines_to_pick >= count:\n",
        "        print(\"Returning all lines\")\n",
        "        return no_empty_string\n",
        "    else:\n",
        "        print(\"Returning {} lines\".format(lines_to_pick))\n",
        "        indexes = default_rng().choice(count - 1, size=lines_to_pick, replace=False)\n",
        "        return [no_empty_string[index] for index in indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERvjEYEMXJML",
        "outputId": "cc8954af-245b-4806-8828-e8fa42256e2a"
      },
      "source": [
        "#num_samples = 1000\n",
        "#linesStarter = preprocess_file_emnlp('data/wikitext-103/wiki.valid.tokens.txt', lines_to_pick=num_samples)\n",
        "#with open('linesStarterWiki103.txt','w+') as f:\n",
        "#    for line in linesStarter:\n",
        "#      f.write(line)\n",
        "#      f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitted on new line\n",
            "Removed empty strings\n",
            "Returning 1000 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZbBx36wAS2G"
      },
      "source": [
        "First 5 rows of starter lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hs5V8FzAJuP",
        "outputId": "9bf15dfa-18b1-458a-af0a-2f58d568909b"
      },
      "source": [
        "import numpy as np\n",
        "transformerXL_sents=list()\n",
        "\n",
        "in_file_starter = 'progettoML/save/tfxl/linesStarterWiki103.txt'\n",
        "lines = [sent.strip() for sent in open(in_file_starter).readlines()]\n",
        "\n",
        "for line in lines[0:5]:\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tikal was not sacked but its power and influence were broken.\n",
            "The Kuma battalion — led by Major Takeshi Mizuno — attacked the southeastern sector of the Lunga perimeter , defended by Marines of the 3rd Battalion , 1st Marine Regiment ( 3 / 1 ).\n",
            "The portion of current Route 50 between Seaville and Petersburg received funding in 1910 to become a spur of the Ocean Highway.\n",
            "The island and its surrounding seas harbour diverse populations of wildlife.\n",
            "He instructed them not to , according to his memoirs , telling them that many rural blacks only assisted the guerrillas under extreme duress , and that it would not do to attack them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cByj8GA7HsE"
      },
      "source": [
        "num_samples = len(lines)\n",
        "\n",
        "count=0\n",
        "top_k = 40\n",
        "max_len = 40\n",
        "\n",
        "for line in lines:\n",
        "    count = count+1\n",
        "    if count % 100 == 0:\n",
        "      print(\"Numero di righe processate:\", count, \"su \", num_samples)\n",
        "    #print('Initial sequence: ' + line)\n",
        "    line_tokenized = tokenizer.tokenize(line)\n",
        "    line_indexed = tokenizer.convert_tokens_to_ids(line_tokenized)\n",
        "    tokens_tensor = torch.tensor([line_indexed])\n",
        "    tokens_tensor = tokens_tensor.to(device)\n",
        "\n",
        "    predicted_tokens=list()\n",
        "    for i in range(max_len):\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs.prediction_scores\n",
        "        # prendo i 100 vocaboli più frequenti\n",
        "        kth_vals, kth_idx = torch.topk(predictions[0, -1, :],k=top_k)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        predicted_index = kth_idx[dist.sample()] \n",
        "        #print(predicted_index)\n",
        "        # ricavo token\n",
        "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "        #print(predicted_token)\n",
        "        predicted_tokens.append(predicted_token)\n",
        "        # break if [EOS] reached\n",
        "        if predicted_token == tokenizer.eos_token:\n",
        "          withoutEos = list(filter(('<eos>').__ne__, predicted_tokens))\n",
        "          if len(withoutEos) >= 4:\n",
        "            break\n",
        "        predicted_index = torch.tensor([[predicted_index]]).to(device)\n",
        "        tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
        "    #print('Predicted output: ' + \" \".join(predicted_tokens))\n",
        "    transformerXL_sents.append(\" \".join(predicted_tokens))\n",
        "\n",
        "out_file_TFXL = \"%s-len%d-topk%d-%s.txt\" % (\"tfxl\", max_len, top_k,\"emnlp\")\n",
        "\n",
        "with open(out_file_TFXL,'w+')as f:\n",
        "    for line in replace(transformerXL_sents):\n",
        "       f.write(line)\n",
        "       f.write(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Qx4jJyLF7J",
        "outputId": "ff7a2eff-2fb0-4f3b-91ab-3e423cccc551"
      },
      "source": [
        "out_file_TFXL = 'progettoML/save/tfxl/tfxl-len40-topk40-wiki103.txt'\n",
        "transformerXL_sents_out = [sent.lower().replace(\"<unk>\",\"[UNK]\").strip().split() for sent in open(out_file_TFXL).readlines()]\n",
        "for sent in transformerXL_sents_out[0:5]:\n",
        "  print(\" \".join(sent))\n",
        "\n",
        "with open(\"tfxl_prepared\",\"w\") as f:\n",
        "  for sent in transformerXL_sents_out:\n",
        "    f.write(\" \".join(sent))\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" the biggest problem in the history of the world is the absence of any meaningful representative representative parliamentary representation , \" said professor mark r. harris . \" there is only one representative representative in the parliament ,\n",
            "the 4th battalion , 1st marines was hit by the japanese attack during the battle of mount austen .\n",
            "during the winter of 1927 , the southern portion of the highway was rerouted .\n",
            "the number of animal species on the island is estimated to be between 4 @,@ 900 and 7 @,@ 000 , most of whom are threatened by human intervention .\n",
            "however , after the battle of the barnfield , the guerrillas broke up and were dispersed . as a result of this battle , many were killed .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y544Z1Lbt4wK",
        "outputId": "15022472-477b-49bb-fdfa-ab21a8810e62"
      },
      "source": [
        "PPL_tfxl = np.average([(perplexity_model.score(\" \".join(sent))['positional_scores'].mean().neg().exp()).item() for sent in transformerXL_sents_out])\n",
        "print(\"TFXL perplexity:\",format(PPL_tfxl,\".4f\"))\n",
        "table.set_cell(item=format(PPL_tfxl,\".4f\"), row_name=\"TFXL\",col_name=\"PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFXL perplexity: 125.6344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiGGU8s0A9TY",
        "outputId": "faad976c-2426-4deb-b75e-8986f867b457"
      },
      "source": [
        "PPL_tfxl_corpus = [perplexity_model.score(\" \".join(sent))['positional_scores'].mean().item() for sent in transformerXL_sents_out]\n",
        "PPL_tfxl_corpus = np.exp(-np.mean(PPL_tfxl_corpus))\n",
        "print(\"TFXL perplexity-corpus:\",format(PPL_tfxl_corpus,\".4f\"))\n",
        "table.set_cell(item=format(PPL_tfxl_corpus,\".4f\"), row_name=\"TFXL\",col_name=\"Corpus-PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFXL perplexity-corpus: 66.6718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "642UBkwBjWv-"
      },
      "source": [
        "# TexyGen\n",
        "\n",
        "---\n",
        "\n",
        "Campioniamo 1000 righe dal training di Wiki103 per avere una baseline per il confronto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZAVxYEP4KQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c17359-b549-4d61-c4d4-fa221234e164"
      },
      "source": [
        "import re\n",
        "from numpy.random import default_rng\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "def split_points(line: str):\n",
        "    regex = re.compile('(\".*?\")|(\\(.*?\\))|(\\[.*?\\])|(\\{.*?\\})')\n",
        "    count = 0\n",
        "    rp = \"xxxxxxxxxxx%d\"%(count)\n",
        "    new_line = line \n",
        "    for match in regex.finditer(line):\n",
        "      new_line=new_line.replace(match.group(),rp)\n",
        "      count = count + 1\n",
        "      rp = \"xxxxxxxxxxx%d\"%(count)\n",
        "    lines = new_line.split(\" . \")\n",
        "    count = 0\n",
        "    rp = \"xxxxxxxxxxx%d\"%(count)\n",
        "    for match in regex.finditer(line):\n",
        "      for i in range(len(lines)):\n",
        "        lines[i]=lines[i].replace(rp,match.group())\n",
        "      count = count + 1\n",
        "      rp = \"xxxxxxxxxxx%d\"%(count)     \n",
        "    return lines\n",
        "\n",
        "def preprocess_wiki(filename, lines_to_pick=-1):\n",
        "  new_lines = list()\n",
        "  count=0\n",
        "  flag = True\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      # remove titles\n",
        "      if len(line.split(\" \")) > 2 and count == 0 and flag:\n",
        "        [new_lines.append(item) for item in tokenize.sent_tokenize(line.replace(\"<unk>\",\"[UNK]\").strip())]\n",
        "        flag = True\n",
        "      elif len(line.split(\" \")) <= 2:\n",
        "        if not flag:\n",
        "          count = 0\n",
        "          flag = True\n",
        "        else:\n",
        "          count = count + 1\n",
        "      elif line.startswith(\" =\"):\n",
        "        count = 0\n",
        "        flag = False\n",
        "    filtered = list()\n",
        "    for line in new_lines:\n",
        "        if line.strip() != \".\":\n",
        "          filtered.append(line)\n",
        "    count = len(filtered)\n",
        "    if lines_to_pick == -1 or lines_to_pick >= count:\n",
        "        print(\"Returning all lines\")\n",
        "        return filtered\n",
        "    else:\n",
        "        print(\"Returning {} lines\".format(lines_to_pick))\n",
        "        indexes = default_rng().choice(count - 1, size=lines_to_pick, replace=False)\n",
        "        return [filtered[index] for index in indexes]\n",
        "    return filtered\n",
        "\n",
        "def preprocess_wiki07(filename, lines_to_pick=-1):\n",
        "  new_lines = list()\n",
        "  count=0\n",
        "  flag = True\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      # remove titles\n",
        "      if len(line.split(\" \")) > 2 and count == 0 and flag:\n",
        "        [new_lines.append(item.replace(\"<unk>\",\"[UNK]\").strip()+\" .\") for item in split_points(line)]\n",
        "        flag = True\n",
        "      elif len(line.split(\" \")) <= 2:\n",
        "        if not flag:\n",
        "          count = 0\n",
        "          flag = True\n",
        "        else:\n",
        "          count = count + 1\n",
        "      elif line.startswith(\" =\"):\n",
        "        count = 0\n",
        "        flag = False\n",
        "    filtered = list()\n",
        "    for line in new_lines:\n",
        "        if line.strip() != \".\":\n",
        "          filtered.append(line)\n",
        "    count = len(filtered)\n",
        "    if lines_to_pick == -1 or lines_to_pick >= count:\n",
        "        print(\"Returning all lines\")\n",
        "        return filtered\n",
        "    else:\n",
        "        print(\"Returning {} lines\".format(lines_to_pick))\n",
        "        indexes = default_rng().choice(count - 1, size=lines_to_pick, replace=False)\n",
        "        return [filtered[index] for index in indexes]\n",
        "    return filtered\n",
        "\n",
        "def preprocess_wiki07(filename, lines_to_pick=-1):\n",
        "  new_lines = list()\n",
        "  count=0\n",
        "  flag = True\n",
        "  with open(filename, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "      # remove titles\n",
        "      if len(line.split(\" \")) > 2 and count == 0 and flag:\n",
        "        [new_lines.append(item.replace(\"<unk>\",\"[UNK]\").strip()+\" .\") for item in split_points(line)]\n",
        "        flag = True\n",
        "      elif len(line.split(\" \")) <= 2:\n",
        "        if not flag:\n",
        "          count = 0\n",
        "          flag = True\n",
        "        else:\n",
        "          count = count + 1\n",
        "      elif line.startswith(\" =\"):\n",
        "        count = 0\n",
        "        flag = False\n",
        "    filtered = list()\n",
        "    for line in new_lines:\n",
        "        if line.strip() != \".\":\n",
        "          filtered.append(line)\n",
        "    count = len(filtered)\n",
        "    if lines_to_pick == -1 or lines_to_pick >= count:\n",
        "        print(\"Returning all lines\")\n",
        "        return filtered\n",
        "    else:\n",
        "        print(\"Returning {} lines\".format(lines_to_pick))\n",
        "        indexes = default_rng().choice(count - 1, size=lines_to_pick, replace=False)\n",
        "        return [filtered[index] for index in indexes]\n",
        "    return filtered\n",
        "\n",
        "#lines = preprocess_wiki('data/wikitext-103/wiki.train.tokens', lines_to_pick=1000)\n",
        "#with open('wiki1000.txt','w+')as f:\n",
        "#    for line in lines:\n",
        "#       f.write(line)\n",
        "#       f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Returning 5000 lines\n",
            "Returning 5000 lines\n",
            "Returning 1000 lines\n",
            "Returning 1000 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7bdfcZtkiVI",
        "outputId": "9875cb29-4ba1-4179-eba4-13b67004eb8e"
      },
      "source": [
        "out_file_WT103 = 'progettoML/save/newWiki1000.txt'\n",
        "WT103_sents_out = [sent.replace(\"<unk>\",\"[UNK]\").strip().split() for sent in open(out_file_WT103).readlines()]\n",
        "for sent in WT103_sents_out[0:5]:\n",
        "  print(\" \".join(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On December 15 , 2011 , a deal was reached on the remaining nine appropriations bills , which were combined into the Consolidated Appropriations Act , 2012 .\n",
            "Io 's volcanism is responsible for many of its unique features .\n",
            "Mycena intersecta is a species of mushroom in the Mycenaceae family .\n",
            "The March 1984 issue of Dragon contained a short story titled \" The Test of the Twins \" by Margaret Weis , along with a sidebar describing Dragonlance as \" an epic adventure usable with the AD & D game system , and will be detailed through a series of TSR products – books , games , modules , and even miniature figures . \" In the same issue a full @-@ page teaser advertisement showed a black @-@ and @-@ white version of the painting from the cover of Dragons of Despair with the text \" Play the epic series ... Advanced Dungeons & Dragons Dragonlance \" .\n",
            "Later in the month the band played second on the bill at the Reading and Leeds Festivals before headlining the Hydro Connect Music Festival in Argyll , Scotland .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v8MNud8CBUE",
        "outputId": "c74f5f4b-bde8-486a-8623-c3c1504e0940"
      },
      "source": [
        "PPL_WT103 = np.average([(perplexity_model.score(\" \".join(sent))['positional_scores'].mean().neg().exp()).item() for sent in WT103_sents_out])\n",
        "print(\"WT103 perplexity:\",format(PPL_WT103,\".4f\"))\n",
        "table.set_cell(item=format(PPL_WT103,\".4f\"), row_name=\"WT103\",col_name=\"PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 perplexity: 186.1305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fmQOAuBCBUG",
        "outputId": "c0b2dfbc-8281-4f4b-b47f-23c5a73897f9"
      },
      "source": [
        "PPL_WT103_corpus = [perplexity_model.score(\" \".join(sent))['positional_scores'].mean().item() for sent in WT103_sents_out]\n",
        "PPL_WT103_corpus = np.exp(-np.mean(PPL_WT103_corpus))\n",
        "print(\"WT103 perplexity-corpus:\",format(PPL_WT103_corpus,\".4f\"))\n",
        "table.set_cell(item=format(PPL_WT103_corpus,\".4f\"), row_name=\"WT103\",col_name=\"Corpus-PPL\")\n",
        "WT103_sents_out = [sent.lower().replace(\"<unk>\",\"[UNK]\").strip().split() for sent in open(out_file_WT103).readlines()]\n",
        "\n",
        "with open('newWiki1000.prepared.txt','w') as f:\n",
        "  for sent in WT103_sents_out:\n",
        "    f.write(\" \".join(sent))\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 perplexity-corpus: 66.3768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBl6XXUnatyk"
      },
      "source": [
        "Selecting 5000 rows by Wiki103 test split as reference test for computing metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45cnmiqOasGT"
      },
      "source": [
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        " \n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "wiki_data = prepare_wiki('progettoML/save/wiki103.5k.txt')\n",
        "\n",
        "with open('wiki103.5k.prepared.txt','w') as f:\n",
        "  for sent in wiki_data:\n",
        "    f.write(\" \".join(sent))\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Mle7psx5nv"
      },
      "source": [
        "## *BLEU score (Bilingual Evaluation Understudy)*\n",
        "\n",
        "It's value is given by\n",
        "\n",
        "\\begin{equation}\n",
        "BLUE=BP \\exp\\left(\\sum_{n=1}^N w_n \\log p_n\\right)\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "\n",
        "*   $p_n$ is the *modified $n$-grams precision*, i.e.\n",
        "\\begin{equation}\n",
        "p_n= \\frac{ \\sum_{C\\in \\{\\text{Candidates}\\}} \\sum_{n\\text{-grams} \\in C}\\text{Count}_{\\text{clipped}}(n\\text{-grams})}{\\sum_{C' \\in \\{\\text{Candidates}\\}} \\sum_{n\\text{-grams}' \\in C'}\\text{Count}(n\\text{-grams}')}\n",
        "\\end{equation}\n",
        "where $\\text{Count}_{\\text{clipped}}(n\\text{-grams})= \\max\\{Count(n\\text{-gram}), \\text{Max_ref_Count}(n\\text{-gram})\\}$.\n",
        "\n",
        "*   $N=4$ means that we condiser $n$-grams up to $N$;\n",
        "\n",
        "*   $w_n$ is the weight for $n$-grams, introduced for taking into account of different size of grams combined. Blue uses uniform weight;\n",
        "\n",
        "*   BP is the *brevity penality* given by:\n",
        "\\begin{equation}\n",
        "\\text{BP}= \\begin{cases} 1 & if & r < c \\\\ e^{1-\\frac{r}{c}} & if & r \\geq c \\end{cases}\n",
        "\\end{equation}\n",
        "where $c$ is the sum of lenghts of cadidates and $r$ is the sum of *best match lengths*. The best match lenght for the single canditate is the closest length of reference to its. This factor allows to penalize sencente brevity. One consideration remains: if we computed the brevity penalty sentence by sentence and averaged the penalties, then length deviations on short sentences would be punished harshly. Instead, we compute the brevity penalty over the entire corpus to allow some freedom at the sentence level. Candidate translations longer than their references are already penalized by the modified $n$-gram precision measure: there is no need to penalize them again. \n",
        "\n",
        "Finally, we high-light that we resorted to a smoothing technique in or-der not to penalize small sentences when consideringhigher  ordern-gram  precision  (e.g.n= 4).  Basically, given a certain $n$-gram, if, when computing its precision,  the  number  of  matchedn-grams $m_n$ is0,the algorithm actually replaces $m_n$ with a small value of $\\epsilon$ (0.1, in our case). Texygen chooses *smoothing method 1*, which replaces 0 value with $\\epsilon=0.1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7ncOCyAh2z-"
      },
      "source": [
        "from nltk.translate import bleu_score as bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated,smoothing_function=SmoothingFunction().method1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBVcGu5jXaqR",
        "outputId": "bc4f79c9-4183-4fc8-b562-dc10137859ac"
      },
      "source": [
        "bleuBERTbase = 100 * corpus_bleu(bert_sents_out, wiki_data)\n",
        "print(\"BERT-base bleu corpus:\",format(bleuBERTbase,\".4f\"))\n",
        "table.set_cell(item=format(bleuBERTbase,\".4f\"), row_name=\"BERT base\",col_name=\"Corpus-BLEU WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-base bleu corpus: 9.0017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H49tQxgiDtMr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4b6559-f4a6-47d4-b3d4-89844fcf15e8"
      },
      "source": [
        "bleuOpenai = 100 * corpus_bleu(openai_sents_out, wiki_data)\n",
        "print(\"OpenAI GPT bleu corpus:\",format(bleuOpenai,\".4f\"))\n",
        "table.set_cell(item=format(bleuOpenai,\".4f\"), row_name=\"GPT\",col_name=\"Corpus-BLEU WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenAI GPT bleu corpus: 11.1340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHwItTUhDtiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85608ccf-1054-492f-815e-e7db6d34669f"
      },
      "source": [
        "bleuTFXL = 100 * corpus_bleu(transformerXL_sents_out, wiki_data)\n",
        "print(\"TFXL bleu corpus:\",format(bleuTFXL,\".4f\"))\n",
        "table.set_cell(item=format(bleuTFXL,\".4f\"), row_name=\"TFXL\",col_name=\"Corpus-BLEU WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFXL bleu corpus: 22.0293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzqhlNZ0Dtww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a36b3ada-7d56-4337-822d-94c32155b4c1"
      },
      "source": [
        "bleuWT103 = 100 * corpus_bleu(WT103_sents_out, wiki_data)\n",
        "print(\"WT103 bleu corpus:\",format(bleuWT103,\".4f\"))\n",
        "table.set_cell(item=format(bleuWT103,\".4f\"), row_name=\"WT103\",col_name=\"Corpus-BLEU WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 bleu corpus: 16.1486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpubYkit-6sQ"
      },
      "source": [
        "## Self - BLEU (Diversity)\n",
        "\n",
        "We propose Self-BLEU, a metric to evaluate the *diversity* of the generated data. Since BLEU aims to assess how similar two sentences are, it can also be used to evaluate how one sentence resembles the rest in a generated collection. Regarding one sentence as hypothesis and the others as reference, we can calculate BLEU score for every generated sentence, and define the average BLEU score to be the Self-BLEU of the document.\n",
        "A higher Self-BLEU score implies less diversity of the document. Lower is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-dM6GIsAD6r"
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def self_bleu(sents):\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents,smoothing_function=SmoothingFunction().method1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_6sKXgz_zMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e090ab-d750-4c70-f6b0-92525ca63f3a"
      },
      "source": [
        "selfbleuBERTbase = 100 * self_bleu(bert_sents_out)\n",
        "print(\"BERT-base self-bleu:\",format(selfbleuBERTbase,\".4f\"))\n",
        "gramTable.set_cell(item=format(selfbleuBERTbase,\".4f\"), row_name=\"BERT base\",col_name=\"Self-BLEU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-base self-bleu: 8.7787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dew5RBn_EkW0",
        "outputId": "6c29353a-a66e-4f7c-e4ea-c9971ac10deb"
      },
      "source": [
        "selfbleuOpenai = 100 * self_bleu(openai_sents_out)\n",
        "print(\"OpenAI GPT self-bleu:\",format(selfbleuOpenai,\".4f\"))\n",
        "gramTable.set_cell(item=format(selfbleuOpenai,\".4f\"), row_name=\"GPT\",col_name=\"Self-BLEU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenAI GPT self-bleu: 39.3600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LVfqLdBEkn9",
        "outputId": "394721f8-fa2f-4277-c037-75f7a35ea36d"
      },
      "source": [
        "selfbleuTransformerXL = 100 * self_bleu(transformerXL_sents_out)\n",
        "print(\"TFXL self-bleu:\",format(selfbleuTransformerXL,\".4f\"))\n",
        "gramTable.set_cell(item=format(selfbleuTransformerXL,\".4f\"), row_name=\"TFXL\",col_name=\"Self-BLEU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFXL self-bleu: 22.1123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaP8xWVYEk7A",
        "outputId": "04e92b2f-b51d-4816-f3eb-48d9f9b11dd1"
      },
      "source": [
        "selfbleuWT103 = 100 * self_bleu(WT103_sents_out)\n",
        "print(\"WT103 self-bleu:\",format(selfbleuWT103,\".4f\"))\n",
        "gramTable.set_cell(item=format(selfbleuWT103,\".4f\"), row_name=\"WT103\",col_name=\"Self-BLEU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 self-bleu: 10.1783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z2EmRIEdH8W"
      },
      "source": [
        "## Unique n-grams\n",
        "\n",
        "For each model, we re-port both the percentage of distinct generatedn-gramsthat do not appear in the reference text and the percent-age of generatedn-grams that appear only one time inthe whole generated text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9_vtSaxdMq9"
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {}\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents if len(sent)>=i for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "\n",
        "def ref_unique_ngrams(preds, refs, max_n=4):\n",
        "    # get # of *distinct* pred ngrams that don't appear in ref\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "        pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "    return pct_unique\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "    return pct_unique"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrTm5p0xCJ92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd239288-2831-4ff4-ea25-863cfc09a71f"
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(bert_sents_out, wiki_data, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"BERT base unique %d-grams relative to Wiki: %.4f\" % (i, 100 * pct_uniques[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques[i],\".4f\"), row_name=\"BERT base\",col_name=\"WT103 unique %d-grams\"%(i))\n",
        "\n",
        "pct_uniques_self = self_unique_ngrams(bert_sents_out, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"BERT base unique %d-grams relative to self: %.4f\" % (i, 100 * pct_uniques_self[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques_self[i],\".4f\"), row_name=\"BERT base\",col_name=\"Self unique %d-grams\"%(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT base unique 2-grams relative to Wiki: 58.5522\n",
            "BERT base unique 3-grams relative to Wiki: 91.8973\n",
            "BERT base unique 4-grams relative to Wiki: 98.6718\n",
            "BERT base unique 2-grams relative to self: 62.7488\n",
            "BERT base unique 3-grams relative to self: 92.5906\n",
            "BERT base unique 4-grams relative to self: 98.4108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyvaYNDqCJ92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74048bc-918b-49ad-cad1-0322bf1dd721"
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques_GPT = ref_unique_ngrams(openai_sents_out, wiki_data, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"OpenAI GPT unique %d-grams relative to Wiki: %.4f\" % (i, 100 * pct_uniques_GPT[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques_GPT[i],\".4f\"), row_name=\"GPT\",col_name=\"WT103 unique %d-grams\"%(i))\n",
        "\n",
        "pct_uniques_GPT_self = self_unique_ngrams(openai_sents_out, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"OpenAI GPT unique %d-grams relative to self: %.4f\" % (i, 100 * pct_uniques_GPT_self[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques_GPT_self[i],\".4f\"), row_name=\"GPT\",col_name=\"Self unique %d-grams\"%(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OpenAI GPT unique 2-grams relative to Wiki: 33.6686\n",
            "OpenAI GPT unique 3-grams relative to Wiki: 73.2306\n",
            "OpenAI GPT unique 4-grams relative to Wiki: 91.2346\n",
            "OpenAI GPT unique 2-grams relative to self: 31.5774\n",
            "OpenAI GPT unique 3-grams relative to self: 67.5123\n",
            "OpenAI GPT unique 4-grams relative to self: 87.6764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x04w1yrE_ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7adcdd87-f237-418c-cc32-34bc5cec613a"
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques_tfxl = ref_unique_ngrams(transformerXL_sents_out, wiki_data, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"TFXL %d-grams relative to Wiki: %.4f\" % (i, 100 * pct_uniques_tfxl[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques_tfxl[i],\".4f\"), row_name=\"TFXL\",col_name=\"WT103 unique %d-grams\"%(i))\n",
        "\n",
        "pct_uniques_self_tfxl = self_unique_ngrams(transformerXL_sents_out, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"TFXL unique %d-grams relative to self: %.4f\" % (i, 100 * pct_uniques_self_tfxl[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques_self_tfxl[i],\".4f\"), row_name=\"TFXL\",col_name=\"Self unique %d-grams\"%(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFXL 2-grams relative to Wiki: 38.6328\n",
            "TFXL 3-grams relative to Wiki: 78.1598\n",
            "TFXL 4-grams relative to Wiki: 94.5532\n",
            "TFXL unique 2-grams relative to self: 48.1990\n",
            "TFXL unique 3-grams relative to self: 82.0207\n",
            "TFXL unique 4-grams relative to self: 95.0062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCIGWnALE_5u",
        "outputId": "31f0f701-04be-4bf3-902a-90797430786b"
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques_WT103 = ref_unique_ngrams(WT103_sents_out, wiki_data, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to Wiki: %.4f\" % (i, 100 * pct_uniques_WT103[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques_WT103[i],\".4f\"), row_name=\"WT103\",col_name=\"WT103 unique %d-grams\"%(i))\n",
        "\n",
        "pct_uniques_self_WT103 = self_unique_ngrams(WT103_sents_out, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to self: %.4f\" % (i, 100 * pct_uniques_self_WT103[i]))\n",
        "    gramTable.set_cell(item =format(100 * pct_uniques_self_WT103[i],\".4f\"), row_name=\"WT103\",col_name=\"Self unique %d-grams\"%(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 unique 2-grams relative to Wiki: 56.3098\n",
            "WT103 unique 3-grams relative to Wiki: 88.4135\n",
            "WT103 unique 4-grams relative to Wiki: 97.6938\n",
            "WT103 unique 2-grams relative to self: 69.1396\n",
            "WT103 unique 3-grams relative to self: 93.8131\n",
            "WT103 unique 4-grams relative to self: 98.8779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myp6esJ6NXMl"
      },
      "source": [
        "## EmbSim (Embedding Similarity)\n",
        "\n",
        "EMbSimaims to quantify the similarity of two distinctwritten  texts  by  comparing  their  word  embeddings.First, word embedding is evaluated on real data using askip-gram model. For each word embedding, we com-pute its cosine distance with the other words, and thenformulate it as a matrixW, whereWij= cos(ei, ej),withei,ej, being the world embeddings of wordsiandjfrom real data.Wis calledsimilarity matrixof thereal data. The similarity matrix of the generated data isdefined analogously:W′ij= cos(e′i, e′j), wheree′i,e′jare the embeddings of wordsi′andj′from generateddata, respectively. The EmbSim is then defined as\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{EmbSim} = \\log \\left(\\sum_{i=1}^N \\frac{\\cos\\left(W_i',W_i\\right)}{N} \\right),\n",
        "\\end{equation}\n",
        "\n",
        "where $N$ is the total number of words and $W_i$ and $W_i'$ denote the $i$-th column of $W$ and $W'$ respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLA1UAUop6Zd"
      },
      "source": [
        "# coding=utf-8\n",
        "import nltk\n",
        "\n",
        "def text_to_code(tokens, dictionary, seq_len):\n",
        "    code_str = \"\"\n",
        "    eof_code = len(dictionary)\n",
        "    for sentence in tokens:\n",
        "        index = 0\n",
        "        for word in sentence:\n",
        "            code_str += (str(dictionary[word]) + ' ')\n",
        "            index += 1\n",
        "        while index < seq_len:\n",
        "            code_str += (str(eof_code) + ' ')\n",
        "            index += 1\n",
        "        code_str += '\\n'\n",
        "    return code_str\n",
        "\n",
        "\n",
        "def code_to_text(codes, dictionary):\n",
        "    paras = \"\"\n",
        "    eof_code = len(dictionary)\n",
        "    for line in codes:\n",
        "        numbers = map(int, line)\n",
        "        for number in numbers:\n",
        "            if number == eof_code:\n",
        "                continue\n",
        "            paras += (dictionary[str(number)] + ' ')\n",
        "        paras += '\\n'\n",
        "    return paras\n",
        "\n",
        "\n",
        "def get_tokenlized(file):\n",
        "    tokenlized = list()\n",
        "    with open(file) as raw:\n",
        "        for text in raw:\n",
        "            text = nltk.word_tokenize(text.lower())\n",
        "            tokenlized.append(text)\n",
        "    return tokenlized\n",
        "\n",
        "\n",
        "def get_word_list(tokens):\n",
        "    word_set = list()\n",
        "    for sentence in tokens:\n",
        "        for word in sentence:\n",
        "            word_set.append(word)\n",
        "    return list(set(word_set))\n",
        "\n",
        "\n",
        "def get_dict(word_set):\n",
        "    word_index_dict = dict()\n",
        "    index_word_dict = dict()\n",
        "    index = 0\n",
        "    for word in word_set:\n",
        "        word_index_dict[word] = str(index)\n",
        "        index_word_dict[str(index)] = word\n",
        "        index += 1\n",
        "    return word_index_dict, index_word_dict\n",
        "\n",
        "def text_precess(train_text_loc, test_text_loc=None):\n",
        "    train_tokens = get_tokenlized(train_text_loc)\n",
        "    if test_text_loc is None:\n",
        "        test_tokens = list()\n",
        "    else:\n",
        "        test_tokens = get_tokenlized(test_text_loc)\n",
        "    word_set = get_word_list(train_tokens + test_tokens)\n",
        "    [word_index_dict, index_word_dict] = get_dict(word_set)\n",
        "\n",
        "    if test_text_loc is None:\n",
        "        sequence_len = len(max(train_tokens, key=len))\n",
        "    else:\n",
        "        sequence_len = max(len(max(train_tokens, key=len)), len(max(test_tokens, key=len)))\n",
        "    with open('eval_data.txt', 'w') as outfile:\n",
        "        outfile.write(text_to_code(test_tokens, word_index_dict, sequence_len))\n",
        "\n",
        "    return sequence_len, len(word_index_dict) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOvHUdGsdqh5",
        "outputId": "95d2b39f-5683-4272-a93d-cf6b25e9b8e2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "sequence_len, vocab_size_BERT = text_precess(out_file_BERT, \"wiki103.5k.prepared.txt\")\n",
        "sequence_len, vocab_size_OPENAI = text_precess(out_file_OPENAI, \"wiki103.5k.prepared.txt\")\n",
        "sequence_len, vocab_size_TFXL = text_precess(\"tfxl_prepared\", \"wiki103.5k.prepared.txt\")\n",
        "sequence_len, vocab_size_WT103 = text_precess('newWiki1000.prepared.txt', \"wiki103.5k.prepared.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfT9PqUWNbGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b65cd6-4b82-4721-c2c4-d348abd0b508"
      },
      "source": [
        "import collections\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "class DocEmbSim():\n",
        "    def __init__(self, oracle_file=None, generator_file=None, num_vocabulary = None):\n",
        "        super().__init__()\n",
        "        self.name = 'EmbeddingSimilarity'\n",
        "        self.oracle_sim = None\n",
        "        self.gen_sim = None\n",
        "        self.is_first = True\n",
        "        self.oracle_file = oracle_file\n",
        "        self.generator_file = generator_file\n",
        "        self.num_vocabulary = num_vocabulary\n",
        "        self.batch_size = 64\n",
        "        self.embedding_size = 32\n",
        "        self.data_index = 0\n",
        "        self.valid_examples = None\n",
        "\n",
        "    def get_score(self):\n",
        "        if self.is_first:\n",
        "            self.get_oracle_sim()\n",
        "            self.is_first = False\n",
        "        self.get_gen_sim()\n",
        "        return self.get_dis_corr()\n",
        "   \n",
        "    def get_tokenlized(self,file):\n",
        "        tokenlized = list()\n",
        "        with open(file) as raw:\n",
        "            for text in raw:\n",
        "                text = nltk.word_tokenize(text.lower())\n",
        "                tokenlized.append(text)\n",
        "        return tokenlized\n",
        "\n",
        "    def text_precess(self,train_text_loc):\n",
        "        train_tokens = get_tokenlized(train_text_loc)\n",
        "        word_set = get_word_list(train_tokens)\n",
        "        [word_index_dict, index_word_dict] = get_dict(word_set)\n",
        "        sequence_len = len(max(train_tokens, key=len))\n",
        "        with open('eval_data.txt', 'w') as outfile:\n",
        "          outfile.write(text_to_code(train_tokens, word_index_dict, sequence_len))\n",
        "\n",
        "    def get_frequent_word(self):\n",
        "        if self.valid_examples is not None:\n",
        "            return self.valid_examples\n",
        "\n",
        "        import collections\n",
        "        self.text_precess(self.oracle_file)\n",
        "        words = []\n",
        "        with open('eval_data.txt', 'r') as file:\n",
        "            for line in file:\n",
        "                text = nltk.word_tokenize(line)\n",
        "                text = list(map(int, text))\n",
        "                words += text\n",
        "        counts = collections.Counter(words)\n",
        "        new_list = sorted(words, key=lambda x: -counts[x])\n",
        "        word_set = list(set(new_list))\n",
        "        if len(word_set) < self.num_vocabulary//10: # // divide e restituisce la parte intera -> floor division\n",
        "            self.valid_examples = word_set\n",
        "            return word_set\n",
        "        else:\n",
        "            self.valid_examples = word_set[0:self.num_vocabulary//10]\n",
        "            return word_set[0:self.num_vocabulary//10]\n",
        "\n",
        "    def read_data(self, file):\n",
        "        self.text_precess(file)\n",
        "        words = []\n",
        "        with open('eval_data.txt', 'r') as file:\n",
        "            for line in file:\n",
        "                text = nltk.word_tokenize(line)\n",
        "                words.append(text)\n",
        "        return words\n",
        "\n",
        "    def generate_batch(self, batch_size, num_skips, skip_window, data=None):\n",
        "        assert batch_size % num_skips == 0\n",
        "        assert num_skips <= 2 * skip_window\n",
        "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "        span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
        "        buffer = collections.deque(maxlen=span)\n",
        "        for _ in range(span):\n",
        "            buffer.append(data[self.data_index])\n",
        "            self.data_index = (self.data_index + 1) % len(data)\n",
        "        for i in range(batch_size // num_skips):\n",
        "            target = skip_window  # target label at the center of the buffer\n",
        "            targets_to_avoid = [skip_window]\n",
        "            for j in range(num_skips):\n",
        "                while target in targets_to_avoid:\n",
        "                    target = random.randint(0, span - 1)\n",
        "                targets_to_avoid.append(target)\n",
        "                batch[i * num_skips + j] = buffer[skip_window]\n",
        "                labels[i * num_skips + j, 0] = buffer[target]\n",
        "            buffer.append(data[self.data_index])\n",
        "            self.data_index = (self.data_index + 1) % len(data)\n",
        "        return batch, labels\n",
        "\n",
        "    def get_wordvec(self, file):\n",
        "        graph = tf.Graph()\n",
        "        batch_size = self.batch_size\n",
        "        embedding_size = self.embedding_size\n",
        "        vocabulary_size = self.num_vocabulary\n",
        "        num_sampled = 64\n",
        "        if num_sampled > vocabulary_size:\n",
        "            num_sampled = vocabulary_size\n",
        "        num_steps = 2\n",
        "        skip_window = 1  # How many words to consider left and right.\n",
        "        num_skips = 2  # How many times to reuse an input to generate a label.\n",
        "        if self.valid_examples is None:\n",
        "            self.get_frequent_word()\n",
        "\n",
        "        with graph.as_default():\n",
        "\n",
        "            # Input data.\n",
        "            train_dataset = tf.compat.v1.placeholder(tf.int32, shape=[batch_size])\n",
        "            train_labels = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "            valid_dataset = tf.compat.v1.constant(self.valid_examples, dtype=tf.int32)\n",
        "\n",
        "            # initial Variables.\n",
        "            embeddings = tf.Variable(\n",
        "                tf.compat.v1.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, seed=11))\n",
        "            softmax_weights = tf.Variable(\n",
        "                tf.compat.v1.truncated_normal([vocabulary_size, embedding_size],\n",
        "                                    stddev=1.0 / math.sqrt(embedding_size), seed=12))\n",
        "            softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "            # Model.\n",
        "            # Look up embeddings for inputs.\n",
        "            embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
        "            # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "            loss = tf.reduce_mean(\n",
        "                tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
        "                                           labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
        "\n",
        "            optimizer = tf.compat.v1.train.AdagradOptimizer(1.0).minimize(loss)\n",
        "\n",
        "            # Compute the similarity between minibatch examples and all embeddings.\n",
        "            norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "            normalized_embeddings = embeddings / norm\n",
        "            valid_embeddings = tf.nn.embedding_lookup(\n",
        "                normalized_embeddings, valid_dataset)\n",
        "            similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n",
        "\n",
        "            data = self.read_data(file)\n",
        "\n",
        "        with tf.compat.v1.Session(graph=graph) as session:\n",
        "            tf.compat.v1.global_variables_initializer().run()\n",
        "            average_loss = 0\n",
        "            generate_num = len(data)\n",
        "            for step in range(num_steps):\n",
        "                for index in range(generate_num):\n",
        "                    cur_batch_data, cur_batch_labels = self.generate_batch(\n",
        "                        batch_size, num_skips, skip_window, data[index])\n",
        "                    feed_dict = {train_dataset: cur_batch_data, train_labels: cur_batch_labels}\n",
        "                    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "                    average_loss += l\n",
        "            similarity_value = similarity.eval()\n",
        "            return similarity_value\n",
        "\n",
        "    def get_oracle_sim(self):\n",
        "        self.oracle_sim = self.get_wordvec(self.oracle_file)\n",
        "\n",
        "    def get_gen_sim(self):\n",
        "        self.gen_sim = self.get_wordvec(self.generator_file)\n",
        "\n",
        "    def get_dis_corr(self):\n",
        "        if len(self.oracle_sim) != len(self.gen_sim):\n",
        "            raise ArithmeticError\n",
        "        corr = 0\n",
        "        for index in range(len(self.oracle_sim)):\n",
        "            corr += (cosine(np.array(self.oracle_sim[index]), np.array(self.gen_sim[index])))\n",
        "        return np.log10(corr / len(self.oracle_sim))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM5Nz1MacuX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dabcd1f-5dd3-4ee9-fc18-ca72d49a60ab"
      },
      "source": [
        "objEmbSimBERTbase = DocEmbSim(oracle_file=out_file_BERT, generator_file=\"wiki103.5k.prepared.txt\", num_vocabulary = vocab_size_BERT)\n",
        "BERTEmbSim = objEmbSimBERTbase.get_score()\n",
        "print(\"BERT base EmbSim: %.4f\" % BERTEmbSim )\n",
        "table.set_cell(item =format(BERTEmbSim,\".4f\"), row_name=\"BERT base\",col_name=\"EmbSim WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT base EmbSim: -2.1656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z5_h_sWEcUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd09cd33-d97d-47de-d5fd-25e1a7aa76ba"
      },
      "source": [
        "objEmbSimOPENAI = DocEmbSim(oracle_file=out_file_OPENAI, generator_file=\"wiki103.5k.prepared.txt\", num_vocabulary = vocab_size_OPENAI)\n",
        "openaiEmbSim = objEmbSimOPENAI.get_score()\n",
        "print(\"GPT EmbSim: %.4f\" % openaiEmbSim )\n",
        "table.set_cell(item =format(openaiEmbSim,\".4f\"), row_name=\"GPT\",col_name=\"EmbSim WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT EmbSim: -1.7255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7virAVDEVwIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a204af-cc90-441c-95aa-1f18ae038135"
      },
      "source": [
        "objEmbSimTFXL = DocEmbSim(oracle_file=\"tfxl_prepared\", generator_file=\"wiki103.5k.prepared.txt\", num_vocabulary = vocab_size_TFXL)\n",
        "tfxlEmbSim = objEmbSimTFXL.get_score()\n",
        "print(\"TFXL EmbSim: %.4f\" % tfxlEmbSim )\n",
        "table.set_cell(item =format(tfxlEmbSim,\".4f\"), row_name=\"TFXL\",col_name=\"EmbSim WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TFXL EmbSim: -2.2129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igFmMLkkii6P",
        "outputId": "b394ae2b-ad31-418c-cf1c-cd721f6912a8"
      },
      "source": [
        "objEmbSimWT103 = DocEmbSim(oracle_file='newWiki1000.prepared.txt', generator_file=\"wiki103.5k.prepared.txt\", num_vocabulary = vocab_size_WT103)\n",
        "WT103EmbSim = objEmbSimWT103.get_score()\n",
        "print(\"WT103 EmbSim: %.4f\" % WT103EmbSim )\n",
        "table.set_cell(item =format(WT103EmbSim,\".4f\"), row_name=\"WT103\",col_name=\"EmbSim WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 EmbSim: -2.4209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUookmIm4AGF"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igifch4i3_av",
        "outputId": "c02a8cab-32d4-4361-abc0-29a82f350691"
      },
      "source": [
        "table.print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------\n",
            "|                 |Corpus-BLEU WT103|       PPL       |   Corpus-PPL    |  EmbSim WT103   |\n",
            "|    BERT base    |     9.0017      |    289.1632     |    212.9936     |     -2.1656     |\n",
            "|       GPT       |     11.1340     |    175.7630     |    155.2120     |     -1.7255     |\n",
            "|      TFXL       |     22.0293     |    125.6344     |     66.6718     |     -2.2129     |\n",
            "|      WT103      |     16.1486     |    186.1305     |     66.3768     |     -2.4209     |\n",
            "-------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-yhm_l04DXV",
        "outputId": "9b595def-b913-4919-98f1-cfba032ed9af"
      },
      "source": [
        "gramTable.print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "|                    |     Self-BLEU      |Self unique 2-grams |Self unique 3-grams |Self unique 4-grams |WT103 unique 2-grams|WT103 unique 3-grams|WT103 unique 4-grams|\n",
            "|     BERT base      |       8.7787       |      62.7488       |      92.5906       |      98.4108       |      58.5522       |      91.8973       |      98.6718       |\n",
            "|        GPT         |      39.3600       |      31.5774       |      67.5123       |      87.6764       |      33.6686       |      73.2306       |      91.2346       |\n",
            "|        TFXL        |      22.1123       |      48.1990       |      82.0207       |      95.0062       |      38.6328       |      78.1598       |      94.5532       |\n",
            "|       WT103        |      10.1783       |      69.1396       |      93.8131       |      98.8779       |      56.3098       |      88.4135       |      97.6938       |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-aGkHlaGlxW"
      },
      "source": [
        "tfxlscores =list()\n",
        "gptscores = list()\n",
        "bertbasescores = list()\n",
        "with open(\"progettoML/fluencyscores.txt\",\"r\") as f:\n",
        "  for line in f.readlines():\n",
        "    ll=line.split()\n",
        "    bertbasescores.append(ll[1])\n",
        "    gptscores.append(ll[2])\n",
        "    tfxlscores.append(ll[3])\n",
        "bertbasescores = bertbasescores[1:]\n",
        "gptscores = gptscores[1:]\n",
        "tfxlscores = tfxlscores[1:]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "FOH5zxenGoIH",
        "outputId": "34c7e083-987d-4f82-ea6d-a76c447de7f3"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "import numpy as np\n",
        "x0 = np.random.randn(500)\n",
        "x1 = np.random.randn(500) + 1\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=bertbasescores,\n",
        "    histnorm='percent',\n",
        "    name='BERT base',\n",
        "    xbins=dict(\n",
        "        start=0.97,\n",
        "        end=4.27,\n",
        "        size=0.33\n",
        "    ),\n",
        "    marker_color='#F9AF08',\n",
        "    opacity=0.75\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=gptscores,\n",
        "    histnorm='percent',\n",
        "    name='GPT', # name used in legend and hover labels\n",
        "    xbins=dict( # bins used for histogram\n",
        "        start=0.97,\n",
        "        end=4.27,\n",
        "        size=0.33\n",
        "    ),\n",
        "    marker_color='#08F91B',\n",
        "    opacity=0.75\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=tfxlscores,\n",
        "    histnorm='percent',\n",
        "    name='TFXL',\n",
        "    xbins=dict(\n",
        "        start=0.97,\n",
        "        end=4.27,\n",
        "        size=0.33\n",
        "    ),\n",
        "    marker_color='#08C4F9',\n",
        "    opacity=0.75\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    #title_text='Sampled Results', # title of plot\n",
        "    yaxis_title_text='Number of sentences', # xaxis label\n",
        "    xaxis_title_text='Score', # yaxis label\n",
        "    bargap=0.2, # gap between bars of adjacent location coordinates\n",
        "    bargroupgap=0.1 # gap between bars of the same location coordinates\n",
        ")\n",
        "\n",
        "fig.update_xaxes(ticktext=[\"\",\"1.0<br> (Not fluent)\", \"1.5\", \"2.0 <br> (Somewhat fluent)\", \"2.5\",\"3.0 <br> (Fluent)\",\"3.5\",\"4.0 <br> (Very fluent)\",\"\"],\n",
        "                 tickvals=[0.5,1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0,4.5])\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"3c10ec2e-2bde-4b7a-8b35-12b3b792ff9d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"3c10ec2e-2bde-4b7a-8b35-12b3b792ff9d\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '3c10ec2e-2bde-4b7a-8b35-12b3b792ff9d',\n",
              "                        [{\"histnorm\": \"percent\", \"marker\": {\"color\": \"#F9AF08\"}, \"name\": \"BERT base\", \"opacity\": 0.75, \"type\": \"histogram\", \"x\": [\"3\", \"2.33\", \"2.66\", \"1.66\", \"3.33\", \"3.66\", \"2\", \"2.66\", \"2\", \"2.33\", \"2.33\", \"2.66\", \"2.66\", \"2.33\", \"1\", \"3\", \"1\", \"3.33\", \"3\", \"2.66\", \"2.33\", \"2\", \"2.33\", \"2\", \"1\", \"3\", \"2\", \"2.33\", \"3.66\", \"3.66\", \"1\", \"2.66\", \"2.33\", \"2\", \"1\", \"3.66\", \"1.33\", \"2\", \"3.33\", \"2.66\", \"3\", \"2.66\", \"2.66\", \"1\", \"2.66\", \"4\", \"1\", \"1\", \"2.66\", \"1\", \"2\", \"3\", \"2\", \"1.66\", \"1.33\", \"1\", \"2.66\", \"2.33\", \"2.33\", \"2\", \"1.66\", \"1\", \"2.33\", \"1\", \"1.66\", \"1.66\", \"1.33\", \"3\", \"1\", \"2.33\", \"2.33\", \"1.66\", \"1.33\", \"1.66\", \"1\", \"1\", \"1\", \"1\", \"1.66\", \"1.33\", \"2\", \"2\", \"1.33\", \"1.33\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"1\", \"1\", \"1.33\", \"1.66\", \"1\", \"1\", \"1.33\", \"2.33\", \"1.66\", \"1\"], \"xbins\": {\"end\": 4.27, \"size\": 0.33, \"start\": 0.97}}, {\"histnorm\": \"percent\", \"marker\": {\"color\": \"#08F91B\"}, \"name\": \"GPT\", \"opacity\": 0.75, \"type\": \"histogram\", \"x\": [\"3\", \"3\", \"2.66\", \"3\", \"2.66\", \"2.66\", \"2.66\", \"2.66\", \"2.66\", \"2.33\", \"1.66\", \"3\", \"2.66\", \"1\", \"2.66\", \"2.33\", \"2.66\", \"2.66\", \"2\", \"2\", \"1.66\", \"2.66\", \"3\", \"3\", \"2\", \"3\", \"2\", \"1.66\", \"2.33\", \"3\", \"2.66\", \"3\", \"3\", \"2.66\", \"2.33\", \"2\", \"3.33\", \"4\", \"2.33\", \"2.66\", \"1.66\", \"2.33\", \"2.66\", \"2.33\", \"3\", \"2.66\", \"1.33\", \"2\", \"2.33\", \"3.33\", \"2\", \"2\", \"2.33\", \"2.66\", \"2.33\", \"2.33\", \"3\", \"3\", \"2.66\", \"2.33\", \"2\", \"3\", \"3\", \"3\", \"2.66\", \"2.66\", \"1.66\", \"3\", \"2\", \"2.33\", \"3\", \"2.33\", \"2\", \"2.66\", \"2.33\", \"3.33\", \"2.66\", \"2\", \"1.66\", \"1\", \"3\", \"3.33\", \"2.66\", \"2\", \"3\", \"2.33\", \"3.66\", \"3.33\", \"2.66\", \"3\", \"2.66\", \"2.33\", \"3\", \"3.33\", \"3.33\", \"2.66\", \"3\", \"2.66\", \"3.66\", \"2\"], \"xbins\": {\"end\": 4.27, \"size\": 0.33, \"start\": 0.97}}, {\"histnorm\": \"percent\", \"marker\": {\"color\": \"#08C4F9\"}, \"name\": \"TFXL\", \"opacity\": 0.75, \"type\": \"histogram\", \"x\": [\"1.66\", \"2\", \"3.66\", \"2.66\", \"3.33\", \"3.66\", \"2.66\", \"1.33\", \"1.66\", \"1.33\", \"2\", \"2.66\", \"1.66\", \"1.66\", \"3.33\", \"1\", \"3.66\", \"2.66\", \"3.33\", \"1.66\", \"2.33\", \"1.33\", \"1.33\", \"1.33\", \"2.66\", \"1\", \"2.66\", \"3\", \"2.66\", \"3\", \"1.66\", \"1\", \"2.66\", \"2\", \"1\", \"3.33\", \"3.33\", \"1.66\", \"2.33\", \"2.66\", \"2.33\", \"3\", \"1.33\", \"1.33\", \"1.33\", \"2.66\", \"1.33\", \"3.33\", \"3.33\", \"1.66\", \"2\", \"2.66\", \"1\", \"1.33\", \"2.33\", \"2\", \"3.33\", \"2.66\", \"2\", \"1\", \"3\", \"1.66\", \"1.66\", \"1\", \"3.33\", \"1.33\", \"1.33\", \"2.33\", \"2.66\", \"2\", \"2.33\", \"1.66\", \"1.33\", \"1.33\", \"1.33\", \"2.33\", \"3\", \"2.33\", \"1.33\", \"3\", \"2.33\", \"1\", \"1\", \"2.66\", \"3.66\", \"2.33\", \"2.33\", \"1.33\", \"1\", \"2.66\", \"1.66\", \"3.66\", \"1.66\", \"2.66\", \"2.33\", \"3\", \"2.66\", \"3\", \"3\", \"1.66\"], \"xbins\": {\"end\": 4.27, \"size\": 0.33, \"start\": 0.97}}],\n",
              "                        {\"bargap\": 0.2, \"bargroupgap\": 0.1, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"ticktext\": [\"\", \"1.0<br> (Not fluent)\", \"1.5\", \"2.0 <br> (Somewhat fluent)\", \"2.5\", \"3.0 <br> (Fluent)\", \"3.5\", \"4.0 <br> (Very fluent)\", \"\"], \"tickvals\": [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5], \"title\": {\"text\": \"Score\"}}, \"yaxis\": {\"title\": {\"text\": \"Number of sentences\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3c10ec2e-2bde-4b7a-8b35-12b3b792ff9d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGS1qM2oWyu9"
      },
      "source": [
        "# VANILLA MLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCLYfr90Y5GO"
      },
      "source": [
        "First select 5000 rows by the 1000 of the train of Vanilla MLE as a basiline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLkGwnCFfQDA"
      },
      "source": [
        "import re\n",
        "from numpy.random import default_rng\n",
        "import numpy as np\n",
        "\n",
        "# [lines_to_pick] indica il numero di righe da restituire\n",
        "# Se supera il numero massimo di righe che sono state preprocessate le ritorna tutte\n",
        "# Se lines_to_pick == -1 le prende tutte in ogni caso\n",
        "def preprocess_file_MLE(filename: str, lines_to_pick=-1):#, min_line_length=7, max_line_length=100):\n",
        "    with open(filename, 'r') as f:\n",
        "        content = f.readlines()\n",
        "    print(\"Initial data:\", len(content))\n",
        "    without_newline = (\"\".join(content)).split(\"\\n\")\n",
        "    print(\"Splitted on new line\")\n",
        "    no_empty_string = [line.strip() for line in without_newline if len(\n",
        "        line.split(\" \")) > 1 and len(line) != 1]\n",
        "    print(\"Removed empty strings\")\n",
        "    \n",
        "    #filtered_unk = [item for item in no_empty_string if \"<unk>\" not in  item.split(\" \")]\n",
        "\n",
        "    #print(\"Filtered for length between {} and {}\".format(\n",
        "    #    min_line_length, max_line_length))\n",
        "\n",
        "    count = len(no_empty_string)\n",
        "    if lines_to_pick == -1 or lines_to_pick >= count:\n",
        "        print(\"Returning all lines\")\n",
        "        return no_empty_string\n",
        "    else:\n",
        "        print(\"Returning {} lines\".format(lines_to_pick))\n",
        "        indexes = default_rng().choice(count - 1, size=lines_to_pick, replace=False)\n",
        "        return [no_empty_string[index] for index in indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct5TAGZwY260",
        "outputId": "669a0174-bc23-43b6-a706-384d4b360e53"
      },
      "source": [
        "#lines = preprocess_file_MLE('progettoML/Texygen-master/data/wiki.train.txt', lines_to_pick=1000)\n",
        "#with open('wikiSelfPointMLE.txt','w+')as f:\n",
        "#    for line in lines:\n",
        "#       f.write(line)\n",
        "#       f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial data: 10000\n",
            "Splitted on new line\n",
            "Removed empty strings\n",
            "Returning 1000 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjwS-omuY261",
        "outputId": "5e8ed7e8-9419-4779-ebb5-647d5289ddaf"
      },
      "source": [
        "out_file_WT103MLE = 'progettoML/Texygen-master/save/wikiSelfPointMLE.txt'\n",
        "WT103MLE_sents_out = [sent.replace(\"<unk>\",\"[UNK]\").strip().split() for sent in open(out_file_WT103MLE).readlines()]\n",
        "for sent in WT103MLE_sents_out[0:5]:\n",
        "  print(\" \".join(sent))\n",
        "\n",
        "with open(\"wikiSelfPointMLE_prepared.txt\",'w') as f:\n",
        "  for sent in WT103MLE_sents_out:\n",
        "    f.write(\" \".join(sent))\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linked metallocenes can also be formed by introducing several [UNK] substituents onto a single cyclopentadienyl ligand ..\n",
            "\" Like the former , Adrien Begrand of PopMatters wrote enthusiastically for both Cavalera Conspiracy and Inflikted ..\n",
            "It has been suggested that the \" spirit hands \" in the séances of Home were made of gloves stuffed with a substance.\n",
            "Evidence during a board of inquiry later showed that it was a friendly fire incident involving U.S. Air Force and U.S. Navy aircraft mistaking the ships for enemy targets.\n",
            "The cave @-@ dwelling predators capture crustaceans under 1 mm ( 0 @.@ 039 in ) long by entangling them with fine threads , digest them by enveloping them with further threads over the course of a few days , and then return to their normal shape ; there is no evidence that they use venom ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGVKtCLnZcw2"
      },
      "source": [
        "Compute perplexity of WT103MLE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ4XfZOSY261",
        "outputId": "3be3c04a-070f-456c-d3a9-bff784300942"
      },
      "source": [
        "PPL_WT103MLE = np.average([(perplexity_model.score(\" \".join(sent))['positional_scores'].mean().neg().exp()).item() for sent in WT103MLE_sents_out])\n",
        "print(\"WT103MLE perplexity:\",format(PPL_WT103MLE,\".4f\"))\n",
        "tableMLE.set_cell(item=format(PPL_WT103MLE,\".4f\"), row_name=\"WT103MLE\",col_name=\"PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103MLE perplexity: 129.5462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVuHLMPCY261",
        "outputId": "9b89e611-8c03-41fe-af0b-d88df0e4df5f"
      },
      "source": [
        "PPL_WT103MLE_corpus = [perplexity_model.score(\" \".join(sent))['positional_scores'].mean().item() for sent in WT103MLE_sents_out]\n",
        "PPL_WT103MLE_corpus = np.exp(-np.mean(PPL_WT103MLE_corpus))\n",
        "print(\"WT103MLE perplexity-corpus:\",format(PPL_WT103MLE_corpus,\".4f\"))\n",
        "tableMLE.set_cell(item=format(PPL_WT103MLE_corpus,\".4f\"), row_name=\"WT103MLE\",col_name=\"Corpus-PPL\")\n",
        "WT103MLE_sents_out = [sent.lower().replace(\"<unk>\",\"[UNK]\").strip().split() for sent in open(out_file_WT103MLE).readlines()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103MLE perplexity-corpus: 71.1355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMjIgBFDbRsT"
      },
      "source": [
        "Import generated sentence by Vanilla MLE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQSWxL8Ee0RO",
        "outputId": "1b053767-5172-4857-adef-42741b4416e4"
      },
      "source": [
        "#lines = preprocess_file_MLE('progettoML/Texygen-master/save/test_fileMLE.txt', lines_to_pick=1000)\n",
        "#with open('generatedMLE1000.txt','w+')as f:\n",
        "#    for line in lines:\n",
        "#       f.write(line)\n",
        "#       f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial data: 9984\n",
            "Splitted on new line\n",
            "Removed empty strings\n",
            "Returning 1000 lines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gURMKeIJbQt2",
        "outputId": "49715cf2-eae0-4049-c38b-07a2abdb4ce1"
      },
      "source": [
        "out_file_MLE = 'progettoML/Texygen-master/save/generatedMLE1000.txt'\n",
        "MLE_sents_out = [sent.replace(\"< unk >\",\"[UNK]\").replace(\"@ . @\",\"@.@\").replace(\"@ , @\",\"@,@\").replace(\"@ - @\",\"@-@\").replace(\"``\",\"\\\"\").strip().split() for sent in open(out_file_MLE).readlines()]\n",
        "for sent in MLE_sents_out[0:5]:\n",
        "  print(\" \".join(sent))\n",
        "\n",
        "with open(\"generatedMLE1000_prepared.txt\",'w') as f:\n",
        "  for sent in MLE_sents_out:\n",
        "    f.write(\" \".join(sent))\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" one of hardcore ray bc , is the bill was unsuccessfully for 18 @.@ 2 km ) market capitalization is affected by ken marino , and tape boy shotgun , indicating that new show that frances is also noted that the film was would be seen .\n",
            "chinese welding also in a series is featured in the traditional tree @-@ mouthed mamba , is something .\n",
            "the agreement five was that he was without impossible , the fate of the world and deliver at a friendly district show and jma upgrading the southwest floppy victory and and aft ) .\n",
            "oxford felt that it is strengthened , along by the supplier of the hand west of his daughter ( debut , and dancers recorded \" , was , with land to be dangerous ..\n",
            "he was too forced middleton much as a clerk of 25 standing against the right master @-@ ten in the abd al @-@ eup and angang @-@ shirts cross southern swimming in finland and of order to the nation more through one moving on too level into a key men 's hollywood .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_3sY3Fyb8rW",
        "outputId": "9243c1b4-0ac0-47a4-cfbb-310ff7468995"
      },
      "source": [
        "PPL_MLE = np.average([(perplexity_model.score(\" \".join(sent))['positional_scores'].mean().neg().exp()).item() for sent in MLE_sents_out])\n",
        "print(\"MLE perplexity:\",format(PPL_MLE,\".4f\"))\n",
        "tableMLE.set_cell(item=format(PPL_MLE,\".4f\"), row_name=\"MLE\",col_name=\"PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE perplexity: 3285.5876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeZQwEKfb8rn",
        "outputId": "e4aefc83-3e22-4513-885b-9285655f7734"
      },
      "source": [
        "PPL_MLE_corpus = [perplexity_model.score(\" \".join(sent))['positional_scores'].mean().item() for sent in MLE_sents_out]\n",
        "PPL_MLE_corpus = np.exp(-np.mean(PPL_MLE_corpus))\n",
        "print(\"MLE perplexity-corpus:\",format(PPL_MLE_corpus,\".4f\"))\n",
        "tableMLE.set_cell(item=format(PPL_MLE_corpus,\".4f\"), row_name=\"MLE\",col_name=\"Corpus-PPL\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE perplexity-corpus: 1649.8054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R_-ecFma699",
        "outputId": "c64c3288-f024-4beb-a9a3-5476f2d13c86"
      },
      "source": [
        "bleuMLE = 100 * corpus_bleu(MLE_sents_out, wiki_data)\n",
        "print(\"MLE bleu corpus:\",format(bleuMLE,\".4f\"))\n",
        "tableMLE.set_cell(item=format(bleuMLE,\".4f\"), row_name=\"MLE\",col_name=\"Corpus-BLEU WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE bleu corpus: 11.5018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaR3I35ta6-Q",
        "outputId": "be360932-55bb-430a-b3c0-8f29028915cd"
      },
      "source": [
        "bleuWT103MLE = 100 * corpus_bleu(WT103MLE_sents_out, wiki_data)\n",
        "print(\"WT103MLE bleu corpus:\",format(bleuWT103MLE,\".4f\"))\n",
        "tableMLE.set_cell(item=format(bleuWT103MLE,\".4f\"), row_name=\"WT103MLE\",col_name=\"Corpus-BLEU WT103\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103MLE bleu corpus: 13.3151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVgrxgpdcRTJ",
        "outputId": "e67b6338-5f9c-4201-a3eb-c400e569449f"
      },
      "source": [
        "selfbleuMLE= 100 * self_bleu(MLE_sents_out)\n",
        "print(\"MLE self-bleu:\",format(selfbleuMLE,\".4f\"))\n",
        "gramTableMLE.set_cell(item=format(selfbleuMLE,\".4f\"), row_name=\"MLE\",col_name=\"Self-BLEU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE self-bleu: 10.2983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pONjyPIZcRTb",
        "outputId": "dbdc1929-4461-4fe3-a0b4-b780335871e9"
      },
      "source": [
        "selfbleuWT103MLE = 100 * self_bleu(WT103MLE_sents_out)\n",
        "print(\"WT103MLE self-bleu:\",format(selfbleuWT103MLE,\".4f\"))\n",
        "gramTableMLE.set_cell(item=format(selfbleuWT103MLE,\".4f\"), row_name=\"WT103MLE\",col_name=\"Self-BLEU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103MLE self-bleu: 10.0048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWRh5MV4cs0t",
        "outputId": "76f8961f-d2ad-46ce-ddff-4a69974388b8"
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques_MLE = ref_unique_ngrams(MLE_sents_out, wiki_data, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"MLE %d-grams relative to Wiki: %.4f\" % (i, 100 * pct_uniques_MLE[i]))\n",
        "    gramTableMLE.set_cell(item =format(100 * pct_uniques_MLE[i],\".4f\"), row_name=\"MLE\",col_name=\"WT103 unique %d-grams\"%(i))\n",
        "\n",
        "pct_uniques_self_MLE = self_unique_ngrams(MLE_sents_out, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"MLE unique %d-grams relative to self: %.4f\" % (i, 100 * pct_uniques_self_MLE[i]))\n",
        "    gramTableMLE.set_cell(item =format(100 * pct_uniques_self_MLE[i],\".4f\"), row_name=\"MLE\",col_name=\"Self unique %d-grams\"%(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE 2-grams relative to Wiki: 57.5716\n",
            "MLE 3-grams relative to Wiki: 91.2157\n",
            "MLE 4-grams relative to Wiki: 99.0350\n",
            "MLE unique 2-grams relative to self: 65.4716\n",
            "MLE unique 3-grams relative to self: 94.0357\n",
            "MLE unique 4-grams relative to self: 99.3602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weyMw7Ykcs0v",
        "outputId": "9fadac8a-e97b-4094-aa0d-d4ed7d6ed722"
      },
      "source": [
        "max_n = 4\n",
        "\n",
        "pct_uniques_WT103MLE = ref_unique_ngrams(WT103MLE_sents_out, wiki_data, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"WT103MLE unique %d-grams relative to Wiki: %.4f\" % (i, 100 * pct_uniques_WT103MLE[i]))\n",
        "    gramTableMLE.set_cell(item =format(100 * pct_uniques_WT103MLE[i],\".4f\"), row_name=\"WT103MLE\",col_name=\"WT103 unique %d-grams\"%(i))\n",
        "\n",
        "pct_uniques_self_WT103MLE = self_unique_ngrams(WT103MLE_sents_out, max_n)\n",
        "for i in range(2, max_n + 1):\n",
        "    print(\"WT103MLE unique %d-grams relative to self: %.4f\" % (i, 100 * pct_uniques_self_WT103MLE[i]))\n",
        "    gramTableMLE.set_cell(item =format(100 * pct_uniques_self_WT103MLE[i],\".4f\"), row_name=\"WT103MLE\",col_name=\"Self unique %d-grams\"%(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103MLE unique 2-grams relative to Wiki: 56.9304\n",
            "WT103MLE unique 3-grams relative to Wiki: 88.4208\n",
            "WT103MLE unique 4-grams relative to Wiki: 97.8288\n",
            "WT103MLE unique 2-grams relative to self: 69.3997\n",
            "WT103MLE unique 3-grams relative to self: 94.1645\n",
            "WT103MLE unique 4-grams relative to self: 98.9331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix4LJZkbd2LS"
      },
      "source": [
        "sequence_len, vocab_size_MLE = text_precess(\"generatedMLE1000_prepared.txt\", \"wiki103.5k.prepared.txt\")\n",
        "sequence_len, vocab_size_WT103MLE = text_precess(\"wikiSelfPointMLE_prepared.txt\", \"wiki103.5k.prepared.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ieh7EZgpd-1z",
        "outputId": "5ae53545-4a3e-4208-f9fe-834c5d7b9706"
      },
      "source": [
        "objEmbSimMLE = DocEmbSim(oracle_file=\"generatedMLE1000_prepared.txt\", generator_file=\"wiki103.5k.prepared.txt\", num_vocabulary = vocab_size_MLE)\n",
        "MLEEmbSim = objEmbSimMLE.get_score()\n",
        "print(\"MLE EmbSim: %.4f\" % MLEEmbSim )\n",
        "tableMLE.set_cell(item =format(MLEEmbSim,\".4f\"), row_name=\"MLE\",col_name=\"EmbSim\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE EmbSim: -2.3916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqkdXd2_d-11",
        "outputId": "8b310c64-1d67-470a-c7e2-81024fa21615"
      },
      "source": [
        "objEmbSimWT103MLE = DocEmbSim(oracle_file=\"wikiSelfPointMLE_prepared.txt\", generator_file=\"wiki103.5k.prepared.txt\", num_vocabulary = vocab_size_WT103MLE)\n",
        "WT103MLEEmbSim = objEmbSimWT103MLE.get_score()\n",
        "print(\"WT103 EmbSim: %.4f\" % WT103MLEEmbSim )\n",
        "tableMLE.set_cell(item =format(WT103MLEEmbSim,\".4f\"), row_name=\"WT103MLE\",col_name=\"EmbSim\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 EmbSim: -2.3850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2FNIzIsdRwD",
        "outputId": "9fdfee27-dd25-4ee2-c45a-3ef7542f9caa"
      },
      "source": [
        "tableMLE.print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------\n",
            "|                 |Corpus-BLEU WT103|       PPL       |   Corpus-PPL    |     EmbSim      |\n",
            "|       MLE       |     11.5018     |    3285.5876    |    1649.8054    |     -2.3916     |\n",
            "|    WT103MLE     |     13.3151     |    129.5462     |     71.1355     |     -2.3850     |\n",
            "-------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qJ35znodXEC",
        "outputId": "c4d74549-72bb-4aeb-820c-fcace8ddc37a"
      },
      "source": [
        "gramTableMLE.print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "|                    |     Self-BLEU      |Self unique 2-grams |Self unique 3-grams |Self unique 4-grams |WT103 unique 2-grams|WT103 unique 3-grams|WT103 unique 4-grams|\n",
            "|        MLE         |      10.2983       |      65.4716       |      94.0357       |      99.3602       |      57.5716       |      91.2157       |      99.0350       |\n",
            "|      WT103MLE      |      10.0048       |      69.3997       |      94.1645       |      98.9331       |      56.9304       |      88.4208       |      97.8288       |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
